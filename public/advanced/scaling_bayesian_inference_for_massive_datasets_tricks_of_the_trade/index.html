<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Data Driven Discovery - D3</title>
<meta name="keywords" content="">
<meta name="description" content="Scaling Bayesian Inference for Massive Datasets: Tricks of the Trade In an era where data is burgeoning at an exponential rate, the demand for robust statistical methods to make sense of this data is more pressing than ever. Among these methods, Bayesian inference stands out for its ability to quantify uncertainty, incorporate prior knowledge, and provide a comprehensive probabilistic framework. However, its applicative prowess is often hindered by computational challenges, especially when dealing with massive datasets.">
<meta name="author" content="">
<link rel="canonical" href="http://example.org/advanced/Scaling_Bayesian_Inference_for_Massive_Datasets_Tricks_of_the_Trade/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://example.org/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://example.org/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://example.org/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://example.org/apple-touch-icon.png">
<link rel="mask-icon" href="http://example.org/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="" />
<meta property="og:description" content="Scaling Bayesian Inference for Massive Datasets: Tricks of the Trade In an era where data is burgeoning at an exponential rate, the demand for robust statistical methods to make sense of this data is more pressing than ever. Among these methods, Bayesian inference stands out for its ability to quantify uncertainty, incorporate prior knowledge, and provide a comprehensive probabilistic framework. However, its applicative prowess is often hindered by computational challenges, especially when dealing with massive datasets." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://example.org/advanced/Scaling_Bayesian_Inference_for_Massive_Datasets_Tricks_of_the_Trade/" /><meta property="article:section" content="advanced" />



<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content=""/>
<meta name="twitter:description" content="Scaling Bayesian Inference for Massive Datasets: Tricks of the Trade In an era where data is burgeoning at an exponential rate, the demand for robust statistical methods to make sense of this data is more pressing than ever. Among these methods, Bayesian inference stands out for its ability to quantify uncertainty, incorporate prior knowledge, and provide a comprehensive probabilistic framework. However, its applicative prowess is often hindered by computational challenges, especially when dealing with massive datasets."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Advanceds",
      "item": "http://example.org/advanced/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "",
      "item": "http://example.org/advanced/Scaling_Bayesian_Inference_for_Massive_Datasets_Tricks_of_the_Trade/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "",
  "name": "",
  "description": "Scaling Bayesian Inference for Massive Datasets: Tricks of the Trade In an era where data is burgeoning at an exponential rate, the demand for robust statistical methods to make sense of this data is more pressing than ever. Among these methods, Bayesian inference stands out for its ability to quantify uncertainty, incorporate prior knowledge, and provide a comprehensive probabilistic framework. However, its applicative prowess is often hindered by computational challenges, especially when dealing with massive datasets.",
  "keywords": [
    
  ],
  "articleBody": "Scaling Bayesian Inference for Massive Datasets: Tricks of the Trade In an era where data is burgeoning at an exponential rate, the demand for robust statistical methods to make sense of this data is more pressing than ever. Among these methods, Bayesian inference stands out for its ability to quantify uncertainty, incorporate prior knowledge, and provide a comprehensive probabilistic framework. However, its applicative prowess is often hindered by computational challenges, especially when dealing with massive datasets. This article aims to demystify the art of scaling Bayesian inference for large datasets, addressing both beginners and advanced practitioners in the fields of Machine Learning, Data Science, and Data Engineering.\nIntroduction Bayesian inference is a powerful statistical paradigm that has found wide application across a diverse set of domains, from genetics to finance. However, its computational cost can become prohibitive with large datasets, making scalability a significant challenge. Fortunately, several strategies have been developed to tackle this issue, allowing for the efficient application of Bayesian methods to big data problems.\nThis article will explore practical, scalable approaches to Bayesian inference, showcasing small working code snippets and advanced tips that may not be widely known. Our focus will be on techniques that can be implemented with commonly used Machine Learning libraries such as TensorFlow, PySpark, and others.\nMain Body Subsampling Methods One of the primary tricks to scale Bayesian inference is to use subsampling methods, which allow the inference process to operate on smaller, randomly selected subsets of the data. This approach significantly reduces computational complexity at the cost of some variance in the estimates.\nimport tensorflow as tf import tensorflow_probability as tfp tfd = tfp.distributions # Generate synthetic data data = tfd.Normal(loc=0., scale=1.).sample(sample_shape=10000) # Subsample data subsampled_data = tf.gather(data, tf.random.shuffle(tf.range(10000))[:1000]) # Bayesian inference with subsampled data model = tfd.Normal(loc=tf.reduce_mean(subsampled_data), scale=tf.math.reduce_std(subsampled_data)) The code above demonstrates how to perform a simple Bayesian inference on a subsample of a normally distributed synthetic dataset using TensorFlow Probability.\nVariational Inference Variational Inference (VI) is another technique to scale Bayesian inference. It turns the inference problem into an optimization problem, approximating the posterior distribution with a simpler distribution by minimizing the Kullback-Leibler (KL) divergence.\nfrom tensorflow_probability import distributions as tfd, vi # Define a simple model model = tfd.Normal(loc=tf.Variable(0., name='loc'), scale=1.) # Define the variational family variational_dist = tfd.Normal(loc=tf.Variable(0., name='q_loc'), scale=tfp.util.TransformedVariable(1., tfp.bijectors.Exp(), name='q_scale')) # Loss function: KL divergence loss = vi.fit_surrogate_posterior(target_log_prob_fn=model.log_prob, surrogate_posterior=variational_dist, optimizer=tf.optimizers.Adam(learning_rate=0.01), num_steps=200) print(\"Variational parameters: \", variational_dist.mean().numpy(), variational_dist.stddev().numpy()) This snippet fits a variational Gaussian approximation to the posterior of a simple model, leveraging TensorFlow Probability’s vi.fit_surrogate_posterior function.\nDistributed Computing with PySpark For truly massive datasets, distributed computing frameworks like Apache Spark can be leveraged to scale Bayesian inference across clusters of machines.\nfrom pyspark.sql import SparkSession from pyspark.mllib.stat import KernelDensity # Initialize Spark session spark = SparkSession.builder.appName(\"BayesianInference\").getOrCreate() # Parallelize data (assuming a RDD of samples) data = spark.sparkContext.parallelize([-2.0, -1.0, 0.0, 1.0, 2.0]) # Kernel density estimation as a form of Bayesian inference kde = KernelDensity() kde.setSample(data) kde.setBandwidth(1.0) # Estimate density at some points estimates = kde.estimate([-1.0, 2.0]) print(estimates) This code employs Apache Spark’s MLlib to perform kernel density estimation, a non-parametric way to infer the underlying probability density function of a dataset.\nConclusion Scaling Bayesian inference for massive datasets involves a blend of statistical techniques and computational strategies. Subsampling allows for working with manageable data sizes, variational inference turns the problem into an optimization task, and distributed computing leverages parallel processing power. By combining these approaches with the power of modern libraries and frameworks, practitioners can overcome the scalability challenges of Bayesian inference, unlocking its full potential for big data applications.\nRemember, the key to successful scaling is to carefully balance accuracy with computational cost, tailoring the approach to the specific needs of your dataset and problem domain. With the tricks and techniques outlined in this article, you’re well-equipped to scale Bayesian inference to meet the challenges of the big data era.\n",
  "wordCount" : "644",
  "inLanguage": "en",
  "datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://example.org/advanced/Scaling_Bayesian_Inference_for_Massive_Datasets_Tricks_of_the_Trade/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Data Driven Discovery - D3",
    "logo": {
      "@type": "ImageObject",
      "url": "http://example.org/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://example.org/" accesskey="h" title="Data Driven Discovery - D3 (Alt + H)">Data Driven Discovery - D3</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      
    </h1>
    <div class="post-meta">

</div>
  </header> 
  <div class="post-content"><h1 id="scaling-bayesian-inference-for-massive-datasets-tricks-of-the-trade">Scaling Bayesian Inference for Massive Datasets: Tricks of the Trade<a hidden class="anchor" aria-hidden="true" href="#scaling-bayesian-inference-for-massive-datasets-tricks-of-the-trade">#</a></h1>
<p>In an era where data is burgeoning at an exponential rate, the demand for robust statistical methods to make sense of this data is more pressing than ever. Among these methods, Bayesian inference stands out for its ability to quantify uncertainty, incorporate prior knowledge, and provide a comprehensive probabilistic framework. However, its applicative prowess is often hindered by computational challenges, especially when dealing with massive datasets. This article aims to demystify the art of scaling Bayesian inference for large datasets, addressing both beginners and advanced practitioners in the fields of Machine Learning, Data Science, and Data Engineering.</p>
<h2 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h2>
<p>Bayesian inference is a powerful statistical paradigm that has found wide application across a diverse set of domains, from genetics to finance. However, its computational cost can become prohibitive with large datasets, making scalability a significant challenge. Fortunately, several strategies have been developed to tackle this issue, allowing for the efficient application of Bayesian methods to big data problems.</p>
<p>This article will explore practical, scalable approaches to Bayesian inference, showcasing small working code snippets and advanced tips that may not be widely known. Our focus will be on techniques that can be implemented with commonly used Machine Learning libraries such as TensorFlow, PySpark, and others.</p>
<h2 id="main-body">Main Body<a hidden class="anchor" aria-hidden="true" href="#main-body">#</a></h2>
<h3 id="subsampling-methods">Subsampling Methods<a hidden class="anchor" aria-hidden="true" href="#subsampling-methods">#</a></h3>
<p>One of the primary tricks to scale Bayesian inference is to use subsampling methods, which allow the inference process to operate on smaller, randomly selected subsets of the data. This approach significantly reduces computational complexity at the cost of some variance in the estimates.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> tensorflow <span style="color:#66d9ef">as</span> tf
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> tensorflow_probability <span style="color:#66d9ef">as</span> tfp
</span></span><span style="display:flex;"><span>tfd <span style="color:#f92672">=</span> tfp<span style="color:#f92672">.</span>distributions
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Generate synthetic data</span>
</span></span><span style="display:flex;"><span>data <span style="color:#f92672">=</span> tfd<span style="color:#f92672">.</span>Normal(loc<span style="color:#f92672">=</span><span style="color:#ae81ff">0.</span>, scale<span style="color:#f92672">=</span><span style="color:#ae81ff">1.</span>)<span style="color:#f92672">.</span>sample(sample_shape<span style="color:#f92672">=</span><span style="color:#ae81ff">10000</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Subsample data</span>
</span></span><span style="display:flex;"><span>subsampled_data <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>gather(data, tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>shuffle(tf<span style="color:#f92672">.</span>range(<span style="color:#ae81ff">10000</span>))[:<span style="color:#ae81ff">1000</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Bayesian inference with subsampled data</span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> tfd<span style="color:#f92672">.</span>Normal(loc<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>reduce_mean(subsampled_data), scale<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>math<span style="color:#f92672">.</span>reduce_std(subsampled_data))
</span></span></code></pre></div><p>The code above demonstrates how to perform a simple Bayesian inference on a subsample of a normally distributed synthetic dataset using TensorFlow Probability.</p>
<h3 id="variational-inference">Variational Inference<a hidden class="anchor" aria-hidden="true" href="#variational-inference">#</a></h3>
<p>Variational Inference (VI) is another technique to scale Bayesian inference. It turns the inference problem into an optimization problem, approximating the posterior distribution with a simpler distribution by minimizing the Kullback-Leibler (KL) divergence.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> tensorflow_probability <span style="color:#f92672">import</span> distributions <span style="color:#66d9ef">as</span> tfd, vi
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Define a simple model</span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> tfd<span style="color:#f92672">.</span>Normal(loc<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>Variable(<span style="color:#ae81ff">0.</span>, name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;loc&#39;</span>), scale<span style="color:#f92672">=</span><span style="color:#ae81ff">1.</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Define the variational family</span>
</span></span><span style="display:flex;"><span>variational_dist <span style="color:#f92672">=</span> tfd<span style="color:#f92672">.</span>Normal(loc<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>Variable(<span style="color:#ae81ff">0.</span>, name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;q_loc&#39;</span>), scale<span style="color:#f92672">=</span>tfp<span style="color:#f92672">.</span>util<span style="color:#f92672">.</span>TransformedVariable(<span style="color:#ae81ff">1.</span>, tfp<span style="color:#f92672">.</span>bijectors<span style="color:#f92672">.</span>Exp(), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;q_scale&#39;</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Loss function: KL divergence</span>
</span></span><span style="display:flex;"><span>loss <span style="color:#f92672">=</span> vi<span style="color:#f92672">.</span>fit_surrogate_posterior(target_log_prob_fn<span style="color:#f92672">=</span>model<span style="color:#f92672">.</span>log_prob,
</span></span><span style="display:flex;"><span>                                  surrogate_posterior<span style="color:#f92672">=</span>variational_dist,
</span></span><span style="display:flex;"><span>                                  optimizer<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>Adam(learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">0.01</span>),
</span></span><span style="display:flex;"><span>                                  num_steps<span style="color:#f92672">=</span><span style="color:#ae81ff">200</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Variational parameters: &#34;</span>, variational_dist<span style="color:#f92672">.</span>mean()<span style="color:#f92672">.</span>numpy(), variational_dist<span style="color:#f92672">.</span>stddev()<span style="color:#f92672">.</span>numpy())
</span></span></code></pre></div><p>This snippet fits a variational Gaussian approximation to the posterior of a simple model, leveraging TensorFlow Probability&rsquo;s <code>vi.fit_surrogate_posterior</code> function.</p>
<h3 id="distributed-computing-with-pyspark">Distributed Computing with PySpark<a hidden class="anchor" aria-hidden="true" href="#distributed-computing-with-pyspark">#</a></h3>
<p>For truly massive datasets, distributed computing frameworks like Apache Spark can be leveraged to scale Bayesian inference across clusters of machines.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> pyspark.sql <span style="color:#f92672">import</span> SparkSession
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> pyspark.mllib.stat <span style="color:#f92672">import</span> KernelDensity
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Initialize Spark session</span>
</span></span><span style="display:flex;"><span>spark <span style="color:#f92672">=</span> SparkSession<span style="color:#f92672">.</span>builder<span style="color:#f92672">.</span>appName(<span style="color:#e6db74">&#34;BayesianInference&#34;</span>)<span style="color:#f92672">.</span>getOrCreate()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Parallelize data (assuming a RDD of samples)</span>
</span></span><span style="display:flex;"><span>data <span style="color:#f92672">=</span> spark<span style="color:#f92672">.</span>sparkContext<span style="color:#f92672">.</span>parallelize([<span style="color:#f92672">-</span><span style="color:#ae81ff">2.0</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1.0</span>, <span style="color:#ae81ff">0.0</span>, <span style="color:#ae81ff">1.0</span>, <span style="color:#ae81ff">2.0</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Kernel density estimation as a form of Bayesian inference</span>
</span></span><span style="display:flex;"><span>kde <span style="color:#f92672">=</span> KernelDensity()
</span></span><span style="display:flex;"><span>kde<span style="color:#f92672">.</span>setSample(data)
</span></span><span style="display:flex;"><span>kde<span style="color:#f92672">.</span>setBandwidth(<span style="color:#ae81ff">1.0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Estimate density at some points</span>
</span></span><span style="display:flex;"><span>estimates <span style="color:#f92672">=</span> kde<span style="color:#f92672">.</span>estimate([<span style="color:#f92672">-</span><span style="color:#ae81ff">1.0</span>, <span style="color:#ae81ff">2.0</span>])
</span></span><span style="display:flex;"><span>print(estimates)
</span></span></code></pre></div><p>This code employs Apache Spark&rsquo;s MLlib to perform kernel density estimation, a non-parametric way to infer the underlying probability density function of a dataset.</p>
<h2 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h2>
<p>Scaling Bayesian inference for massive datasets involves a blend of statistical techniques and computational strategies. Subsampling allows for working with manageable data sizes, variational inference turns the problem into an optimization task, and distributed computing leverages parallel processing power. By combining these approaches with the power of modern libraries and frameworks, practitioners can overcome the scalability challenges of Bayesian inference, unlocking its full potential for big data applications.</p>
<p>Remember, the key to successful scaling is to carefully balance accuracy with computational cost, tailoring the approach to the specific needs of your dataset and problem domain. With the tricks and techniques outlined in this article, you&rsquo;re well-equipped to scale Bayesian inference to meet the challenges of the big data era.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="http://example.org/">Data Driven Discovery - D3</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>

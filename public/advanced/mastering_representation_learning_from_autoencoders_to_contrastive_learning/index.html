<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Mastering Representation Learning: From Autoencoders to Contrastive Learning | Data Driven Discovery - D3</title>
<meta name="keywords" content="Machine Learning, Representation Learning, Advanced Topic">
<meta name="description" content="Mastering Representation Learning: From Autoencoders to Contrastive Learning Representation learning has become a cornerstone of modern machine learning, enabling algorithms to process, interpret, and make predictions from complex data without the need for manual feature engineering. In this comprehensive guide, we&rsquo;ll journey through the fascinating world of representation learning, starting with the fundamentals of autoencoders and advancing to the cutting-edge techniques in contrastive learning. This article is designed to provide actionable insights for both beginners and advanced practitioners in the domains of machine learning, data science, and related fields.">
<meta name="author" content="">
<link rel="canonical" href="https://lustrous-paprenjak-b7c3d8.netlify.app/advanced/Mastering_Representation_Learning_From_Autoencoders_to_Contrastive_Learning/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://lustrous-paprenjak-b7c3d8.netlify.app/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://lustrous-paprenjak-b7c3d8.netlify.app/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://lustrous-paprenjak-b7c3d8.netlify.app/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://lustrous-paprenjak-b7c3d8.netlify.app/apple-touch-icon.png">
<link rel="mask-icon" href="https://lustrous-paprenjak-b7c3d8.netlify.app/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Mastering Representation Learning: From Autoencoders to Contrastive Learning" />
<meta property="og:description" content="Mastering Representation Learning: From Autoencoders to Contrastive Learning Representation learning has become a cornerstone of modern machine learning, enabling algorithms to process, interpret, and make predictions from complex data without the need for manual feature engineering. In this comprehensive guide, we&rsquo;ll journey through the fascinating world of representation learning, starting with the fundamentals of autoencoders and advancing to the cutting-edge techniques in contrastive learning. This article is designed to provide actionable insights for both beginners and advanced practitioners in the domains of machine learning, data science, and related fields." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://lustrous-paprenjak-b7c3d8.netlify.app/advanced/Mastering_Representation_Learning_From_Autoencoders_to_Contrastive_Learning/" /><meta property="article:section" content="advanced" />
<meta property="article:published_time" content="2024-02-05T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-02-05T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Mastering Representation Learning: From Autoencoders to Contrastive Learning"/>
<meta name="twitter:description" content="Mastering Representation Learning: From Autoencoders to Contrastive Learning Representation learning has become a cornerstone of modern machine learning, enabling algorithms to process, interpret, and make predictions from complex data without the need for manual feature engineering. In this comprehensive guide, we&rsquo;ll journey through the fascinating world of representation learning, starting with the fundamentals of autoencoders and advancing to the cutting-edge techniques in contrastive learning. This article is designed to provide actionable insights for both beginners and advanced practitioners in the domains of machine learning, data science, and related fields."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Advanceds",
      "item": "https://lustrous-paprenjak-b7c3d8.netlify.app/advanced/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Mastering Representation Learning: From Autoencoders to Contrastive Learning",
      "item": "https://lustrous-paprenjak-b7c3d8.netlify.app/advanced/Mastering_Representation_Learning_From_Autoencoders_to_Contrastive_Learning/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Mastering Representation Learning: From Autoencoders to Contrastive Learning",
  "name": "Mastering Representation Learning: From Autoencoders to Contrastive Learning",
  "description": "Mastering Representation Learning: From Autoencoders to Contrastive Learning Representation learning has become a cornerstone of modern machine learning, enabling algorithms to process, interpret, and make predictions from complex data without the need for manual feature engineering. In this comprehensive guide, we\u0026rsquo;ll journey through the fascinating world of representation learning, starting with the fundamentals of autoencoders and advancing to the cutting-edge techniques in contrastive learning. This article is designed to provide actionable insights for both beginners and advanced practitioners in the domains of machine learning, data science, and related fields.",
  "keywords": [
    "Machine Learning", "Representation Learning", "Advanced Topic"
  ],
  "articleBody": "Mastering Representation Learning: From Autoencoders to Contrastive Learning Representation learning has become a cornerstone of modern machine learning, enabling algorithms to process, interpret, and make predictions from complex data without the need for manual feature engineering. In this comprehensive guide, we’ll journey through the fascinating world of representation learning, starting with the fundamentals of autoencoders and advancing to the cutting-edge techniques in contrastive learning. This article is designed to provide actionable insights for both beginners and advanced practitioners in the domains of machine learning, data science, and related fields.\nIntroduction to Representation Learning Representation learning is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data. This is especially useful in domains where manual feature engineering is difficult or infeasible. By learning features that capture underlying patterns in the data, models can perform better on a variety of tasks, such as image recognition, natural language processing, and time series analysis.\nAutoencoders: The Building Blocks of Representation Learning Autoencoders are a type of artificial neural network used to learn efficient codings of unlabeled data. The goal of an autoencoder is to compress the input data into a latent-space representation and then reconstruct the input data from this representation. This process forces the autoencoder to capture the most important features in the latent representation.\nImplementing a Simple Autoencoder with TensorFlow Let’s implement a simple autoencoder for image reconstruction using TensorFlow, a popular open-source machine learning library:\nimport tensorflow as tf from tensorflow.keras.layers import Input, Dense, Flatten, Reshape from tensorflow.keras.models import Model # Define the encoder input_img = Input(shape=(28, 28)) # Example for MNIST dataset flattened = Flatten()(input_img) encoded = Dense(128, activation='relu')(flattened) # Define the decoder decoded = Dense(784, activation='sigmoid')(encoded) decoded = Reshape((28, 28))(decoded) # Build the autoencoder model autoencoder = Model(input_img, decoded) autoencoder.compile(optimizer='adam', loss='binary_crossentropy') # Summary of the autoencoder model autoencoder.summary() For training and evaluation, you would proceed by fitting the model on your dataset, typically using the same input and output to learn the reconstruction:\nautoencoder.fit(x_train, x_train, epochs=50, batch_size=256, shuffle=True, validation_data=(x_test, x_test)) Autoencoders can be extended to more complex architectures for specific applications, like Convolutional Autoencoders for image data, Recurrent Autoencoders for sequence data, and Variational Autoencoders (VAEs) for generating new data instances.\nAdvancing to Contrastive Learning Contrastive learning is a technique used in self-supervised learning that trains models to distinguish between similar and dissimilar data points. It does this by learning representations in which similar data points are closer together, and dissimilar points are further apart in the embedding space.\nUnderstanding Contrastive Loss The core idea behind contrastive learning is to use a contrastive loss (or similarity loss) function that encourages the model to learn embeddings such that similar (or “positive”) pairs of data points are pulled closer, and dissimilar (or “negative”) pairs are pushed apart.\nImplementing Contrastive Learning with TensorFlow Here’s a simple example of how to implement a contrastive learning framework for image classification using TensorFlow:\nimport tensorflow as tf def contrastive_loss(y_true, y_pred): \"\"\" Contrastive loss function. \"\"\" margin = 1 square_pred = tf.square(y_pred) margin_square = tf.square(tf.maximum(margin - y_pred, 0)) return tf.reduce_mean(y_true * square_pred + (1 - y_true) * margin_square) # Assuming `anchor`, `positive` and `negative` are the output embeddings from your model loss = contrastive_loss(anchor, positive, negative) This simplistic example highlights the principle behind contrastive loss. In a real-world scenario, you would have a sophisticated model that processes pairs or triplets of inputs to compute their embeddings, and the loss would be computed based on these embeddings to train the model.\nConclusion Representation learning, spanning from autoencoders to contrastive learning, provides powerful tools for feature learning from raw data. Autoencoders help in learning efficient data codings, while contrastive learning techniques push the boundaries by learning embeddings that effectively capture the similarities and dissimilarities among data points. As machine learning continues to evolve, mastering these representation learning techniques will be crucial for developing more sophisticated and efficient models. By understanding and applying the concepts and code examples provided, beginners and advanced users alike can enhance their machine learning projects with effective feature learning capabilities.\nWhether you are just starting out in the field of machine learning or are looking to dive deeper into specific techniques like representation learning, this guide offers foundational insights and practical code examples to help you on your journey. As the field continues to advance, staying updated on the latest techniques and best practices will ensure that your machine learning models remain competitive and effective in tackling complex real-world problems.\nRemember, the key to mastering representation learning is continuous practice and exploration. Dive into projects, experiment with different architectures and datasets, and share your findings with the community. Happy learning!\n",
  "wordCount" : "778",
  "inLanguage": "en",
  "datePublished": "2024-02-05T00:00:00Z",
  "dateModified": "2024-02-05T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://lustrous-paprenjak-b7c3d8.netlify.app/advanced/Mastering_Representation_Learning_From_Autoencoders_to_Contrastive_Learning/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Data Driven Discovery - D3",
    "logo": {
      "@type": "ImageObject",
      "url": "https://lustrous-paprenjak-b7c3d8.netlify.app/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://lustrous-paprenjak-b7c3d8.netlify.app/" accesskey="h" title="Data Driven Discovery - D3 (Alt + H)">Data Driven Discovery - D3</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Mastering Representation Learning: From Autoencoders to Contrastive Learning
    </h1>
    <div class="post-meta"><span title='2024-02-05 00:00:00 +0000 UTC'>February 5, 2024</span>

</div>
  </header> 
  <div class="post-content"><h1 id="mastering-representation-learning-from-autoencoders-to-contrastive-learning">Mastering Representation Learning: From Autoencoders to Contrastive Learning<a hidden class="anchor" aria-hidden="true" href="#mastering-representation-learning-from-autoencoders-to-contrastive-learning">#</a></h1>
<p>Representation learning has become a cornerstone of modern machine learning, enabling algorithms to process, interpret, and make predictions from complex data without the need for manual feature engineering. In this comprehensive guide, we&rsquo;ll journey through the fascinating world of representation learning, starting with the fundamentals of autoencoders and advancing to the cutting-edge techniques in contrastive learning. This article is designed to provide actionable insights for both beginners and advanced practitioners in the domains of machine learning, data science, and related fields.</p>
<h2 id="introduction-to-representation-learning">Introduction to Representation Learning<a hidden class="anchor" aria-hidden="true" href="#introduction-to-representation-learning">#</a></h2>
<p>Representation learning is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data. This is especially useful in domains where manual feature engineering is difficult or infeasible. By learning features that capture underlying patterns in the data, models can perform better on a variety of tasks, such as image recognition, natural language processing, and time series analysis.</p>
<h2 id="autoencoders-the-building-blocks-of-representation-learning">Autoencoders: The Building Blocks of Representation Learning<a hidden class="anchor" aria-hidden="true" href="#autoencoders-the-building-blocks-of-representation-learning">#</a></h2>
<p>Autoencoders are a type of artificial neural network used to learn efficient codings of unlabeled data. The goal of an autoencoder is to compress the input data into a latent-space representation and then reconstruct the input data from this representation. This process forces the autoencoder to capture the most important features in the latent representation.</p>
<h3 id="implementing-a-simple-autoencoder-with-tensorflow">Implementing a Simple Autoencoder with TensorFlow<a hidden class="anchor" aria-hidden="true" href="#implementing-a-simple-autoencoder-with-tensorflow">#</a></h3>
<p>Let&rsquo;s implement a simple autoencoder for image reconstruction using TensorFlow, a popular open-source machine learning library:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> tensorflow <span style="color:#66d9ef">as</span> tf
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> tensorflow.keras.layers <span style="color:#f92672">import</span> Input, Dense, Flatten, Reshape
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> tensorflow.keras.models <span style="color:#f92672">import</span> Model
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Define the encoder</span>
</span></span><span style="display:flex;"><span>input_img <span style="color:#f92672">=</span> Input(shape<span style="color:#f92672">=</span>(<span style="color:#ae81ff">28</span>, <span style="color:#ae81ff">28</span>))  <span style="color:#75715e"># Example for MNIST dataset</span>
</span></span><span style="display:flex;"><span>flattened <span style="color:#f92672">=</span> Flatten()(input_img)
</span></span><span style="display:flex;"><span>encoded <span style="color:#f92672">=</span> Dense(<span style="color:#ae81ff">128</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>)(flattened)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Define the decoder</span>
</span></span><span style="display:flex;"><span>decoded <span style="color:#f92672">=</span> Dense(<span style="color:#ae81ff">784</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;sigmoid&#39;</span>)(encoded)
</span></span><span style="display:flex;"><span>decoded <span style="color:#f92672">=</span> Reshape((<span style="color:#ae81ff">28</span>, <span style="color:#ae81ff">28</span>))(decoded)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Build the autoencoder model</span>
</span></span><span style="display:flex;"><span>autoencoder <span style="color:#f92672">=</span> Model(input_img, decoded)
</span></span><span style="display:flex;"><span>autoencoder<span style="color:#f92672">.</span>compile(optimizer<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;adam&#39;</span>, loss<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;binary_crossentropy&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Summary of the autoencoder model</span>
</span></span><span style="display:flex;"><span>autoencoder<span style="color:#f92672">.</span>summary()
</span></span></code></pre></div><p>For training and evaluation, you would proceed by fitting the model on your dataset, typically using the same input and output to learn the reconstruction:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>autoencoder<span style="color:#f92672">.</span>fit(x_train, x_train,
</span></span><span style="display:flex;"><span>                epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>,
</span></span><span style="display:flex;"><span>                batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>,
</span></span><span style="display:flex;"><span>                shuffle<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>                validation_data<span style="color:#f92672">=</span>(x_test, x_test))
</span></span></code></pre></div><p>Autoencoders can be extended to more complex architectures for specific applications, like Convolutional Autoencoders for image data, Recurrent Autoencoders for sequence data, and Variational Autoencoders (VAEs) for generating new data instances.</p>
<h2 id="advancing-to-contrastive-learning">Advancing to Contrastive Learning<a hidden class="anchor" aria-hidden="true" href="#advancing-to-contrastive-learning">#</a></h2>
<p>Contrastive learning is a technique used in self-supervised learning that trains models to distinguish between similar and dissimilar data points. It does this by learning representations in which similar data points are closer together, and dissimilar points are further apart in the embedding space.</p>
<h3 id="understanding-contrastive-loss">Understanding Contrastive Loss<a hidden class="anchor" aria-hidden="true" href="#understanding-contrastive-loss">#</a></h3>
<p>The core idea behind contrastive learning is to use a contrastive loss (or similarity loss) function that encourages the model to learn embeddings such that similar (or &ldquo;positive&rdquo;) pairs of data points are pulled closer, and dissimilar (or &ldquo;negative&rdquo;) pairs are pushed apart.</p>
<h3 id="implementing-contrastive-learning-with-tensorflow">Implementing Contrastive Learning with TensorFlow<a hidden class="anchor" aria-hidden="true" href="#implementing-contrastive-learning-with-tensorflow">#</a></h3>
<p>Here&rsquo;s a simple example of how to implement a contrastive learning framework for image classification using TensorFlow:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> tensorflow <span style="color:#66d9ef">as</span> tf
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">contrastive_loss</span>(y_true, y_pred):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Contrastive loss function.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    margin <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>    square_pred <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>square(y_pred)
</span></span><span style="display:flex;"><span>    margin_square <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>square(tf<span style="color:#f92672">.</span>maximum(margin <span style="color:#f92672">-</span> y_pred, <span style="color:#ae81ff">0</span>))
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> tf<span style="color:#f92672">.</span>reduce_mean(y_true <span style="color:#f92672">*</span> square_pred <span style="color:#f92672">+</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> y_true) <span style="color:#f92672">*</span> margin_square)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Assuming `anchor`, `positive` and `negative` are the output embeddings from your model</span>
</span></span><span style="display:flex;"><span>loss <span style="color:#f92672">=</span> contrastive_loss(anchor, positive, negative)
</span></span></code></pre></div><p>This simplistic example highlights the principle behind contrastive loss. In a real-world scenario, you would have a sophisticated model that processes pairs or triplets of inputs to compute their embeddings, and the loss would be computed based on these embeddings to train the model.</p>
<h2 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h2>
<p>Representation learning, spanning from autoencoders to contrastive learning, provides powerful tools for feature learning from raw data. Autoencoders help in learning efficient data codings, while contrastive learning techniques push the boundaries by learning embeddings that effectively capture the similarities and dissimilarities among data points. As machine learning continues to evolve, mastering these representation learning techniques will be crucial for developing more sophisticated and efficient models. By understanding and applying the concepts and code examples provided, beginners and advanced users alike can enhance their machine learning projects with effective feature learning capabilities.</p>
<p>Whether you are just starting out in the field of machine learning or are looking to dive deeper into specific techniques like representation learning, this guide offers foundational insights and practical code examples to help you on your journey. As the field continues to advance, staying updated on the latest techniques and best practices will ensure that your machine learning models remain competitive and effective in tackling complex real-world problems.</p>
<p>Remember, the key to mastering representation learning is continuous practice and exploration. Dive into projects, experiment with different architectures and datasets, and share your findings with the community. Happy learning!</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://lustrous-paprenjak-b7c3d8.netlify.app/tags/Machine-Learning/">Machine Learning</a></li>
      <li><a href="https://lustrous-paprenjak-b7c3d8.netlify.app/tags/Representation-Learning/">Representation Learning</a></li>
      <li><a href="https://lustrous-paprenjak-b7c3d8.netlify.app/tags/Advanced-Topic/">Advanced Topic</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://lustrous-paprenjak-b7c3d8.netlify.app/">Data Driven Discovery - D3</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>

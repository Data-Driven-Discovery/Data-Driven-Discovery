<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Beyond Accuracy: Advanced Metrics for Evaluating Machine Learning Models | Data Driven Discovery - D3</title>
<meta name="keywords" content="Machine Learning, Data Analysis, Advanced Topic">
<meta name="description" content="Beyond Accuracy: Advanced Metrics for Evaluating Machine Learning Models When it comes to evaluating machine learning models, accuracy is often the first metric that comes to mind. However, depending solely on accuracy to measure the performance of a model can be misleading, especially in cases where the dataset is imbalanced or the cost of false positives is significantly different from the cost of false negatives. In this article, we dive deep into advanced metrics beyond accuracy that provide a more nuanced understanding of model performance.">
<meta name="author" content="">
<link rel="canonical" href="https://lustrous-paprenjak-b7c3d8.netlify.app/advanced/Beyond_Accuracy_Advanced_Metrics_for_Evaluating_Machine_Learning_Models/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://lustrous-paprenjak-b7c3d8.netlify.app/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://lustrous-paprenjak-b7c3d8.netlify.app/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://lustrous-paprenjak-b7c3d8.netlify.app/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://lustrous-paprenjak-b7c3d8.netlify.app/apple-touch-icon.png">
<link rel="mask-icon" href="https://lustrous-paprenjak-b7c3d8.netlify.app/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Beyond Accuracy: Advanced Metrics for Evaluating Machine Learning Models" />
<meta property="og:description" content="Beyond Accuracy: Advanced Metrics for Evaluating Machine Learning Models When it comes to evaluating machine learning models, accuracy is often the first metric that comes to mind. However, depending solely on accuracy to measure the performance of a model can be misleading, especially in cases where the dataset is imbalanced or the cost of false positives is significantly different from the cost of false negatives. In this article, we dive deep into advanced metrics beyond accuracy that provide a more nuanced understanding of model performance." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://lustrous-paprenjak-b7c3d8.netlify.app/advanced/Beyond_Accuracy_Advanced_Metrics_for_Evaluating_Machine_Learning_Models/" /><meta property="article:section" content="advanced" />
<meta property="article:published_time" content="2024-02-05T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-02-05T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Beyond Accuracy: Advanced Metrics for Evaluating Machine Learning Models"/>
<meta name="twitter:description" content="Beyond Accuracy: Advanced Metrics for Evaluating Machine Learning Models When it comes to evaluating machine learning models, accuracy is often the first metric that comes to mind. However, depending solely on accuracy to measure the performance of a model can be misleading, especially in cases where the dataset is imbalanced or the cost of false positives is significantly different from the cost of false negatives. In this article, we dive deep into advanced metrics beyond accuracy that provide a more nuanced understanding of model performance."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Advanceds",
      "item": "https://lustrous-paprenjak-b7c3d8.netlify.app/advanced/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Beyond Accuracy: Advanced Metrics for Evaluating Machine Learning Models",
      "item": "https://lustrous-paprenjak-b7c3d8.netlify.app/advanced/Beyond_Accuracy_Advanced_Metrics_for_Evaluating_Machine_Learning_Models/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Beyond Accuracy: Advanced Metrics for Evaluating Machine Learning Models",
  "name": "Beyond Accuracy: Advanced Metrics for Evaluating Machine Learning Models",
  "description": "Beyond Accuracy: Advanced Metrics for Evaluating Machine Learning Models When it comes to evaluating machine learning models, accuracy is often the first metric that comes to mind. However, depending solely on accuracy to measure the performance of a model can be misleading, especially in cases where the dataset is imbalanced or the cost of false positives is significantly different from the cost of false negatives. In this article, we dive deep into advanced metrics beyond accuracy that provide a more nuanced understanding of model performance.",
  "keywords": [
    "Machine Learning", "Data Analysis", "Advanced Topic"
  ],
  "articleBody": "Beyond Accuracy: Advanced Metrics for Evaluating Machine Learning Models When it comes to evaluating machine learning models, accuracy is often the first metric that comes to mind. However, depending solely on accuracy to measure the performance of a model can be misleading, especially in cases where the dataset is imbalanced or the cost of false positives is significantly different from the cost of false negatives. In this article, we dive deep into advanced metrics beyond accuracy that provide a more nuanced understanding of model performance. These insights are invaluable for data scientists, data engineers, and MLOps professionals aiming to develop robust machine learning systems.\nIntroduction Accuracy, while a useful metric, does not tell the full story. Imagine a dataset where 95% of the instances belong to one class. A naive model that always predicts this dominant class will achieve 95% accuracy, despite not having learned anything meaningful. This scenario underscores the necessity of exploring additional metrics that can provide a holistic view of a model’s performance. We’ll cover confusion matrix-derived metrics, ROC and AUC, precision-recall curves, and recently, metrics like F1 Score, Balanced Accuracy, and Matthews Correlation Coefficient (MCC).\nMain Body Confusion Matrix and Derived Metrics A confusion matrix is a table used to describe the performance of a classification model. It presents the true classes versus the predicted classes, allowing us to calculate various performance metrics.\nTrue Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN) True Positives (TP): Predictions correctly labeled as positive True Negatives (TN): Predictions correctly labeled as negative False Positives (FP): Negative instances incorrectly labeled as positive False Negatives (FN): Positive instances incorrectly labeled as negative From these values, we can compute the following metrics:\nPrecision, Recall, and F1 Score Precision: Of all the instances predicted as positive, precision measures how many of them were actually positive. def precision(TP, FP): return TP / (TP + FP) # Example precision_val = precision(30, 10) print(f\"Precision: {precision_val:.2f}\") Recall (Sensitivity): Of all the actual positive instances, recall measures how many of them were predicted as positive. def recall(TP, FN): return TP / (TP + FN) # Example recall_val = recall(30, 5) print(f\"Recall: {recall_val:.2f}\") F1 Score: The harmonic mean of precision and recall, providing a balance between the two. def f1_score(precision, recall): return 2 * (precision * recall) / (precision + recall) # Example f1 = f1_score(precision_val, recall_val) print(f\"F1 Score: {f1:.2f}\") Output:\nPrecision: 0.75 Recall: 0.86 F1 Score: 0.80 ROC Curve and AUC The Receiver Operating Characteristic (ROC) curve is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. The Area Under the Curve (AUC) represents a model’s ability to discriminate between positive and negative classes.\nfrom sklearn.metrics import roc_curve, auc import matplotlib.pyplot as plt # Assuming y_test is your test set labels and y_scores are the probabilities predicted by your model fpr, tpr, thresholds = roc_curve(y_test, y_scores) roc_auc = auc(fpr, tpr) plt.figure() plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})') plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--') plt.xlim([0.0, 1.0]) plt.ylim([0.0, 1.05]) plt.xlabel('False Positive Rate') plt.ylabel('True Positive Rate') plt.title('Receiver Operating Characteristic') plt.legend(loc=\"lower right\") plt.show() Precision-Recall Curve For certain applications where the positive class is much smaller than the negative class, the precision-recall curve is a more appropriate metric than the ROC curve.\nfrom sklearn.metrics import precision_recall_curve from sklearn.metrics import plot_precision_recall_curve import matplotlib.pyplot as plt # Assuming clf is your trained model and X_test is the test data disp = plot_precision_recall_curve(clf, X_test, y_test) disp.ax_.set_title('2-class Precision-Recall curve') plt.show() Matthews Correlation Coefficient (MCC) The Matthews Correlation Coefficient is a measure of the quality of binary classifications. It takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes.\nfrom sklearn.metrics import matthews_corrcoef y_true = [1, 0, 1, 1, 0, 1] y_pred = [1, 0, 0, 1, 0, 1] mcc = matthews_corrcoef(y_true, y_pred) print(f\"Matthews Correlation Coefficient: {mcc:.2f}\") Output:\nMatthews Correlation Coefficient: 0.58 Conclusion Evaluating machine learning models goes beyond mere accuracy. A comprehensive evaluation strategy involves a suite of metrics, each providing a unique perspective on the model’s performance. Precision, recall, F1 score, ROC-AUC, and MCC offer deeper insights, especially in scenarios with imbalanced datasets or when the costs of false positives and false negatives differ significantly. By leveraging these metrics, data professionals can develop more reliable and robust machine learning models, ensuring they deliver genuine value in real-world applications. Remember, the choice of metric should align with your project’s specific context and objectives, guiding you towards making informed decisions throughout the model development process.\n",
  "wordCount" : "764",
  "inLanguage": "en",
  "datePublished": "2024-02-05T00:00:00Z",
  "dateModified": "2024-02-05T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://lustrous-paprenjak-b7c3d8.netlify.app/advanced/Beyond_Accuracy_Advanced_Metrics_for_Evaluating_Machine_Learning_Models/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Data Driven Discovery - D3",
    "logo": {
      "@type": "ImageObject",
      "url": "https://lustrous-paprenjak-b7c3d8.netlify.app/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://lustrous-paprenjak-b7c3d8.netlify.app/" accesskey="h" title="Data Driven Discovery - D3 (Alt + H)">Data Driven Discovery - D3</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Beyond Accuracy: Advanced Metrics for Evaluating Machine Learning Models
    </h1>
    <div class="post-meta"><span title='2024-02-05 00:00:00 +0000 UTC'>February 5, 2024</span>

</div>
  </header> 
  <div class="post-content"><h1 id="beyond-accuracy-advanced-metrics-for-evaluating-machine-learning-models">Beyond Accuracy: Advanced Metrics for Evaluating Machine Learning Models<a hidden class="anchor" aria-hidden="true" href="#beyond-accuracy-advanced-metrics-for-evaluating-machine-learning-models">#</a></h1>
<p>When it comes to evaluating machine learning models, accuracy is often the first metric that comes to mind. However, depending solely on accuracy to measure the performance of a model can be misleading, especially in cases where the dataset is imbalanced or the cost of false positives is significantly different from the cost of false negatives. In this article, we dive deep into advanced metrics beyond accuracy that provide a more nuanced understanding of model performance. These insights are invaluable for data scientists, data engineers, and MLOps professionals aiming to develop robust machine learning systems.</p>
<h2 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h2>
<p>Accuracy, while a useful metric, does not tell the full story. Imagine a dataset where 95% of the instances belong to one class. A naive model that always predicts this dominant class will achieve 95% accuracy, despite not having learned anything meaningful. This scenario underscores the necessity of exploring additional metrics that can provide a holistic view of a model&rsquo;s performance. We&rsquo;ll cover confusion matrix-derived metrics, ROC and AUC, precision-recall curves, and recently, metrics like F1 Score, Balanced Accuracy, and Matthews Correlation Coefficient (MCC).</p>
<h2 id="main-body">Main Body<a hidden class="anchor" aria-hidden="true" href="#main-body">#</a></h2>
<h3 id="confusion-matrix-and-derived-metrics">Confusion Matrix and Derived Metrics<a hidden class="anchor" aria-hidden="true" href="#confusion-matrix-and-derived-metrics">#</a></h3>
<p>A confusion matrix is a table used to describe the performance of a classification model. It presents the true classes versus the predicted classes, allowing us to calculate various performance metrics.</p>
<h4 id="true-positives-tp-true-negatives-tn-false-positives-fp-and-false-negatives-fn">True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN)<a hidden class="anchor" aria-hidden="true" href="#true-positives-tp-true-negatives-tn-false-positives-fp-and-false-negatives-fn">#</a></h4>
<ul>
<li><strong>True Positives (TP):</strong> Predictions correctly labeled as positive</li>
<li><strong>True Negatives (TN):</strong> Predictions correctly labeled as negative</li>
<li><strong>False Positives (FP):</strong> Negative instances incorrectly labeled as positive</li>
<li><strong>False Negatives (FN):</strong> Positive instances incorrectly labeled as negative</li>
</ul>
<p>From these values, we can compute the following metrics:</p>
<h4 id="precision-recall-and-f1-score">Precision, Recall, and F1 Score<a hidden class="anchor" aria-hidden="true" href="#precision-recall-and-f1-score">#</a></h4>
<ul>
<li><strong>Precision:</strong> Of all the instances predicted as positive, precision measures how many of them were actually positive.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">precision</span>(TP, FP):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> TP <span style="color:#f92672">/</span> (TP <span style="color:#f92672">+</span> FP)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Example</span>
</span></span><span style="display:flex;"><span>precision_val <span style="color:#f92672">=</span> precision(<span style="color:#ae81ff">30</span>, <span style="color:#ae81ff">10</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Precision: </span><span style="color:#e6db74">{</span>precision_val<span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><ul>
<li><strong>Recall (Sensitivity):</strong> Of all the actual positive instances, recall measures how many of them were predicted as positive.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">recall</span>(TP, FN):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> TP <span style="color:#f92672">/</span> (TP <span style="color:#f92672">+</span> FN)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Example</span>
</span></span><span style="display:flex;"><span>recall_val <span style="color:#f92672">=</span> recall(<span style="color:#ae81ff">30</span>, <span style="color:#ae81ff">5</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Recall: </span><span style="color:#e6db74">{</span>recall_val<span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><ul>
<li><strong>F1 Score:</strong> The harmonic mean of precision and recall, providing a balance between the two.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">f1_score</span>(precision, recall):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> (precision <span style="color:#f92672">*</span> recall) <span style="color:#f92672">/</span> (precision <span style="color:#f92672">+</span> recall)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Example</span>
</span></span><span style="display:flex;"><span>f1 <span style="color:#f92672">=</span> f1_score(precision_val, recall_val)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;F1 Score: </span><span style="color:#e6db74">{</span>f1<span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><p><strong>Output:</strong></p>
<pre tabindex="0"><code>Precision: 0.75
Recall: 0.86
F1 Score: 0.80
</code></pre><h3 id="roc-curve-and-auc">ROC Curve and AUC<a hidden class="anchor" aria-hidden="true" href="#roc-curve-and-auc">#</a></h3>
<p>The Receiver Operating Characteristic (ROC) curve is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. The Area Under the Curve (AUC) represents a model&rsquo;s ability to discriminate between positive and negative classes.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.metrics <span style="color:#f92672">import</span> roc_curve, auc
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Assuming y_test is your test set labels and y_scores are the probabilities predicted by your model</span>
</span></span><span style="display:flex;"><span>fpr, tpr, thresholds <span style="color:#f92672">=</span> roc_curve(y_test, y_scores)
</span></span><span style="display:flex;"><span>roc_auc <span style="color:#f92672">=</span> auc(fpr, tpr)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure()
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>plot(fpr, tpr, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;darkorange&#39;</span>, lw<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;ROC curve (AUC = </span><span style="color:#e6db74">{</span>roc_auc<span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">)&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>plot([<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>], [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>], color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;navy&#39;</span>, lw<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, linestyle<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;--&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlim([<span style="color:#ae81ff">0.0</span>, <span style="color:#ae81ff">1.0</span>])
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>ylim([<span style="color:#ae81ff">0.0</span>, <span style="color:#ae81ff">1.05</span>])
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;False Positive Rate&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;True Positive Rate&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;Receiver Operating Characteristic&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>legend(loc<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;lower right&#34;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><h3 id="precision-recall-curve">Precision-Recall Curve<a hidden class="anchor" aria-hidden="true" href="#precision-recall-curve">#</a></h3>
<p>For certain applications where the positive class is much smaller than the negative class, the precision-recall curve is a more appropriate metric than the ROC curve.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.metrics <span style="color:#f92672">import</span> precision_recall_curve
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.metrics <span style="color:#f92672">import</span> plot_precision_recall_curve
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Assuming clf is your trained model and X_test is the test data</span>
</span></span><span style="display:flex;"><span>disp <span style="color:#f92672">=</span> plot_precision_recall_curve(clf, X_test, y_test)
</span></span><span style="display:flex;"><span>disp<span style="color:#f92672">.</span>ax_<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#39;2-class Precision-Recall curve&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><h3 id="matthews-correlation-coefficient-mcc">Matthews Correlation Coefficient (MCC)<a hidden class="anchor" aria-hidden="true" href="#matthews-correlation-coefficient-mcc">#</a></h3>
<p>The Matthews Correlation Coefficient is a measure of the quality of binary classifications. It takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.metrics <span style="color:#f92672">import</span> matthews_corrcoef
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>y_true <span style="color:#f92672">=</span> [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>y_pred <span style="color:#f92672">=</span> [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>mcc <span style="color:#f92672">=</span> matthews_corrcoef(y_true, y_pred)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Matthews Correlation Coefficient: </span><span style="color:#e6db74">{</span>mcc<span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><p><strong>Output:</strong></p>
<pre tabindex="0"><code>Matthews Correlation Coefficient: 0.58
</code></pre><h2 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h2>
<p>Evaluating machine learning models goes beyond mere accuracy. A comprehensive evaluation strategy involves a suite of metrics, each providing a unique perspective on the model&rsquo;s performance. Precision, recall, F1 score, ROC-AUC, and MCC offer deeper insights, especially in scenarios with imbalanced datasets or when the costs of false positives and false negatives differ significantly. By leveraging these metrics, data professionals can develop more reliable and robust machine learning models, ensuring they deliver genuine value in real-world applications. Remember, the choice of metric should align with your project&rsquo;s specific context and objectives, guiding you towards making informed decisions throughout the model development process.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://lustrous-paprenjak-b7c3d8.netlify.app/tags/Machine-Learning/">Machine Learning</a></li>
      <li><a href="https://lustrous-paprenjak-b7c3d8.netlify.app/tags/Data-Analysis/">Data Analysis</a></li>
      <li><a href="https://lustrous-paprenjak-b7c3d8.netlify.app/tags/Advanced-Topic/">Advanced Topic</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://lustrous-paprenjak-b7c3d8.netlify.app/">Data Driven Discovery - D3</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>

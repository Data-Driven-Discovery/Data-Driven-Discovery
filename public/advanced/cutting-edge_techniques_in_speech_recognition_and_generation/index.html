<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Cutting-Edge Techniques in Speech Recognition and Generation | Data Driven Discovery - D3</title>
<meta name="keywords" content="Natural Language Processing, Deep Learning, Advanced Topic">
<meta name="description" content="Cutting-Edge Techniques in Speech Recognition and Generation In the evolving landscape of artificial intelligence, one of the most dynamic sectors is speech technology. It encompasses both speech recognition—converting spoken language into text—and speech generation—synthesizing human-like speech from text. This article delves into the cutting-edge techniques that are reshaping speech technology, offering insights not only for beginners but also for advanced practitioners in the field.
Introduction The journey of speech technology has been marked by significant milestones, from the early days of rule-based systems to the current era of machine learning and deep neural networks.">
<meta name="author" content="">
<link rel="canonical" href="https://lustrous-paprenjak-b7c3d8.netlify.app/advanced/Cutting-Edge_Techniques_in_Speech_Recognition_and_Generation/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://lustrous-paprenjak-b7c3d8.netlify.app/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://lustrous-paprenjak-b7c3d8.netlify.app/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://lustrous-paprenjak-b7c3d8.netlify.app/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://lustrous-paprenjak-b7c3d8.netlify.app/apple-touch-icon.png">
<link rel="mask-icon" href="https://lustrous-paprenjak-b7c3d8.netlify.app/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Cutting-Edge Techniques in Speech Recognition and Generation" />
<meta property="og:description" content="Cutting-Edge Techniques in Speech Recognition and Generation In the evolving landscape of artificial intelligence, one of the most dynamic sectors is speech technology. It encompasses both speech recognition—converting spoken language into text—and speech generation—synthesizing human-like speech from text. This article delves into the cutting-edge techniques that are reshaping speech technology, offering insights not only for beginners but also for advanced practitioners in the field.
Introduction The journey of speech technology has been marked by significant milestones, from the early days of rule-based systems to the current era of machine learning and deep neural networks." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://lustrous-paprenjak-b7c3d8.netlify.app/advanced/Cutting-Edge_Techniques_in_Speech_Recognition_and_Generation/" /><meta property="article:section" content="advanced" />
<meta property="article:published_time" content="2024-02-05T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-02-05T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Cutting-Edge Techniques in Speech Recognition and Generation"/>
<meta name="twitter:description" content="Cutting-Edge Techniques in Speech Recognition and Generation In the evolving landscape of artificial intelligence, one of the most dynamic sectors is speech technology. It encompasses both speech recognition—converting spoken language into text—and speech generation—synthesizing human-like speech from text. This article delves into the cutting-edge techniques that are reshaping speech technology, offering insights not only for beginners but also for advanced practitioners in the field.
Introduction The journey of speech technology has been marked by significant milestones, from the early days of rule-based systems to the current era of machine learning and deep neural networks."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Advanceds",
      "item": "https://lustrous-paprenjak-b7c3d8.netlify.app/advanced/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Cutting-Edge Techniques in Speech Recognition and Generation",
      "item": "https://lustrous-paprenjak-b7c3d8.netlify.app/advanced/Cutting-Edge_Techniques_in_Speech_Recognition_and_Generation/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Cutting-Edge Techniques in Speech Recognition and Generation",
  "name": "Cutting-Edge Techniques in Speech Recognition and Generation",
  "description": "Cutting-Edge Techniques in Speech Recognition and Generation In the evolving landscape of artificial intelligence, one of the most dynamic sectors is speech technology. It encompasses both speech recognition—converting spoken language into text—and speech generation—synthesizing human-like speech from text. This article delves into the cutting-edge techniques that are reshaping speech technology, offering insights not only for beginners but also for advanced practitioners in the field.\nIntroduction The journey of speech technology has been marked by significant milestones, from the early days of rule-based systems to the current era of machine learning and deep neural networks.",
  "keywords": [
    "Natural Language Processing", "Deep Learning", "Advanced Topic"
  ],
  "articleBody": "Cutting-Edge Techniques in Speech Recognition and Generation In the evolving landscape of artificial intelligence, one of the most dynamic sectors is speech technology. It encompasses both speech recognition—converting spoken language into text—and speech generation—synthesizing human-like speech from text. This article delves into the cutting-edge techniques that are reshaping speech technology, offering insights not only for beginners but also for advanced practitioners in the field.\nIntroduction The journey of speech technology has been marked by significant milestones, from the early days of rule-based systems to the current era of machine learning and deep neural networks. Today, speech technology is integral to various applications, including virtual assistants, automatic transcription services, and voice-activated control systems. The ongoing research and development in this area aim to enhance the accuracy, naturalness, and responsiveness of these systems.\nMain Body Advanced Models in Speech Recognition The most prevalent approach in modern speech recognition involves deep learning, particularly the use of Recurrent Neural Networks (RNNs) and its variants like LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Units). These models are adept at handling sequential data, making them well-suited for the temporal nature of speech.\nimport numpy as np from keras.models import Sequential from keras.layers import LSTM, Dense # Assume X_train and y_train are pre-processed datasets # X_train: The training data, y_train: The labels model = Sequential() model.add(LSTM(128, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2]))) model.add(Dense(y_train.shape[1], activation='softmax')) model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) model.summary() # Train the model model.fit(X_train, y_train, epochs=10, batch_size=64) This basic example illustrates how to define and compile an LSTM model for speech recognition using the Keras library. Note that X_train and y_train would typically be sequences of audio features and corresponding transcriptions, respectively.\nEnhancements in Speech Generation Speech generation, or Text-to-Speech (TTS), has made leaps forward with the advent of models like Tacotron and WaveNet. WaveNet, in particular, is known for producing highly natural speech. It employs a deep convolutional neural network architecture that directly models audio waveforms.\n# Pseudo-code, as implementing WaveNet from scratch is complex and beyond the scope of this example. import tensorflow as tf from tensorflow_tts.models import TFWaveNet # Suppose we have pre-processed text input text_input = \"...\" # Initialize WaveNet model (note: simplified for illustration) wave_net_model = TFWaveNet( config={\"...\" : \"...\"} # Configuration specific to the desired voice characteristics ) # Generate speech audio waveform audio_output = wave_net_model(text_input) # Save or process the audio_output as needed This snippet is simplified and mainly for illustrative purposes. WaveNet and similar models require detailed configuration and extensive training data to produce high-quality speech audio.\nThe Frontier: End-to-End Models The most recent advancement in speech technology is the development of end-to-end models capable of direct translation from speech to text (and vice versa) without intermediate representations. These models, such as DeepSpeech by Mozilla and the Seq2Seq framework used in Google’s Speech-to-Text API, are pushing the boundaries of what’s possible in speech recognition and generation.\n# Again, simplified pseudo-code from deepspeech import Model # Load a pre-trained DeepSpeech model ds_model = Model('path_to_model.pbmm') # Assuming we have an audio file ready for transcription transcription = ds_model.stt('path_to_audio_file.wav') print(transcription) This code illustrates how one might use a pre-trained DeepSpeech model to transcribe audio. Real-world implementation involves more steps, especially for preprocessing the audio data.\nAddressing Challenges Despite the advancements, there remain challenges in speech technology, such as handling diverse accents, noise robustness, and emotional inflection. Researchers are exploring multimodal systems that use additional data sources (e.g., facial expressions, context) to improve understanding and generation.\nConclusion The field of speech technology is at an exciting juncture, with groundbreaking models and techniques continually emerging. For developers and researchers in machine learning, staying updated with these advancements is crucial. By exploring and implementing these cutting-edge techniques, one can contribute to creating more efficient, natural, and accessible speech-based applications.\nThis article has provided a snapshot of the current state of speech recognition and generation, highlighting some of the advanced models and techniques reshaping this domain. As we move forward, the boundaries of what machines can understand and how they communicate back to us will continue to expand, blurring the line between human and machine interaction.\n",
  "wordCount" : "677",
  "inLanguage": "en",
  "datePublished": "2024-02-05T00:00:00Z",
  "dateModified": "2024-02-05T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://lustrous-paprenjak-b7c3d8.netlify.app/advanced/Cutting-Edge_Techniques_in_Speech_Recognition_and_Generation/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Data Driven Discovery - D3",
    "logo": {
      "@type": "ImageObject",
      "url": "https://lustrous-paprenjak-b7c3d8.netlify.app/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://lustrous-paprenjak-b7c3d8.netlify.app/" accesskey="h" title="Data Driven Discovery - D3 (Alt + H)">Data Driven Discovery - D3</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Cutting-Edge Techniques in Speech Recognition and Generation
    </h1>
    <div class="post-meta"><span title='2024-02-05 00:00:00 +0000 UTC'>February 5, 2024</span>

</div>
  </header> 
  <div class="post-content"><h1 id="cutting-edge-techniques-in-speech-recognition-and-generation">Cutting-Edge Techniques in Speech Recognition and Generation<a hidden class="anchor" aria-hidden="true" href="#cutting-edge-techniques-in-speech-recognition-and-generation">#</a></h1>
<p>In the evolving landscape of artificial intelligence, one of the most dynamic sectors is speech technology. It encompasses both speech recognition—converting spoken language into text—and speech generation—synthesizing human-like speech from text. This article delves into the cutting-edge techniques that are reshaping speech technology, offering insights not only for beginners but also for advanced practitioners in the field.</p>
<h2 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h2>
<p>The journey of speech technology has been marked by significant milestones, from the early days of rule-based systems to the current era of machine learning and deep neural networks. Today, speech technology is integral to various applications, including virtual assistants, automatic transcription services, and voice-activated control systems. The ongoing research and development in this area aim to enhance the accuracy, naturalness, and responsiveness of these systems.</p>
<h2 id="main-body">Main Body<a hidden class="anchor" aria-hidden="true" href="#main-body">#</a></h2>
<h3 id="advanced-models-in-speech-recognition">Advanced Models in Speech Recognition<a hidden class="anchor" aria-hidden="true" href="#advanced-models-in-speech-recognition">#</a></h3>
<p>The most prevalent approach in modern speech recognition involves deep learning, particularly the use of Recurrent Neural Networks (RNNs) and its variants like LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Units). These models are adept at handling sequential data, making them well-suited for the temporal nature of speech.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> keras.models <span style="color:#f92672">import</span> Sequential
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> keras.layers <span style="color:#f92672">import</span> LSTM, Dense
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Assume X_train and y_train are pre-processed datasets</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># X_train: The training data, y_train: The labels</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> Sequential()
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>add(LSTM(<span style="color:#ae81ff">128</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>, input_shape<span style="color:#f92672">=</span>(X_train<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>], X_train<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">2</span>])))
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>add(Dense(y_train<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>], activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;softmax&#39;</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>compile(loss<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;categorical_crossentropy&#39;</span>, optimizer<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;adam&#39;</span>, metrics<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;accuracy&#39;</span>])
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>summary()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Train the model</span>
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>fit(X_train, y_train, epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>, batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span>)
</span></span></code></pre></div><p>This basic example illustrates how to define and compile an LSTM model for speech recognition using the Keras library. Note that <code>X_train</code> and <code>y_train</code> would typically be sequences of audio features and corresponding transcriptions, respectively.</p>
<h3 id="enhancements-in-speech-generation">Enhancements in Speech Generation<a hidden class="anchor" aria-hidden="true" href="#enhancements-in-speech-generation">#</a></h3>
<p>Speech generation, or Text-to-Speech (TTS), has made leaps forward with the advent of models like Tacotron and WaveNet. WaveNet, in particular, is known for producing highly natural speech. It employs a deep convolutional neural network architecture that directly models audio waveforms.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Pseudo-code, as implementing WaveNet from scratch is complex and beyond the scope of this example.</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> tensorflow <span style="color:#66d9ef">as</span> tf
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> tensorflow_tts.models <span style="color:#f92672">import</span> TFWaveNet
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Suppose we have pre-processed text input</span>
</span></span><span style="display:flex;"><span>text_input <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;...&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Initialize WaveNet model (note: simplified for illustration)</span>
</span></span><span style="display:flex;"><span>wave_net_model <span style="color:#f92672">=</span> TFWaveNet(
</span></span><span style="display:flex;"><span>    config<span style="color:#f92672">=</span>{<span style="color:#e6db74">&#34;...&#34;</span> : <span style="color:#e6db74">&#34;...&#34;</span>} <span style="color:#75715e"># Configuration specific to the desired voice characteristics</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Generate speech audio waveform</span>
</span></span><span style="display:flex;"><span>audio_output <span style="color:#f92672">=</span> wave_net_model(text_input)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Save or process the audio_output as needed</span>
</span></span></code></pre></div><p>This snippet is simplified and mainly for illustrative purposes. WaveNet and similar models require detailed configuration and extensive training data to produce high-quality speech audio.</p>
<h3 id="the-frontier-end-to-end-models">The Frontier: End-to-End Models<a hidden class="anchor" aria-hidden="true" href="#the-frontier-end-to-end-models">#</a></h3>
<p>The most recent advancement in speech technology is the development of end-to-end models capable of direct translation from speech to text (and vice versa) without intermediate representations. These models, such as DeepSpeech by Mozilla and the Seq2Seq framework used in Google&rsquo;s Speech-to-Text API, are pushing the boundaries of what&rsquo;s possible in speech recognition and generation.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Again, simplified pseudo-code</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> deepspeech <span style="color:#f92672">import</span> Model
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Load a pre-trained DeepSpeech model</span>
</span></span><span style="display:flex;"><span>ds_model <span style="color:#f92672">=</span> Model(<span style="color:#e6db74">&#39;path_to_model.pbmm&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Assuming we have an audio file ready for transcription</span>
</span></span><span style="display:flex;"><span>transcription <span style="color:#f92672">=</span> ds_model<span style="color:#f92672">.</span>stt(<span style="color:#e6db74">&#39;path_to_audio_file.wav&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(transcription)
</span></span></code></pre></div><p>This code illustrates how one might use a pre-trained DeepSpeech model to transcribe audio. Real-world implementation involves more steps, especially for preprocessing the audio data.</p>
<h3 id="addressing-challenges">Addressing Challenges<a hidden class="anchor" aria-hidden="true" href="#addressing-challenges">#</a></h3>
<p>Despite the advancements, there remain challenges in speech technology, such as handling diverse accents, noise robustness, and emotional inflection. Researchers are exploring multimodal systems that use additional data sources (e.g., facial expressions, context) to improve understanding and generation.</p>
<h2 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h2>
<p>The field of speech technology is at an exciting juncture, with groundbreaking models and techniques continually emerging. For developers and researchers in machine learning, staying updated with these advancements is crucial. By exploring and implementing these cutting-edge techniques, one can contribute to creating more efficient, natural, and accessible speech-based applications.</p>
<p>This article has provided a snapshot of the current state of speech recognition and generation, highlighting some of the advanced models and techniques reshaping this domain. As we move forward, the boundaries of what machines can understand and how they communicate back to us will continue to expand, blurring the line between human and machine interaction.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://lustrous-paprenjak-b7c3d8.netlify.app/tags/Natural-Language-Processing/">Natural Language Processing</a></li>
      <li><a href="https://lustrous-paprenjak-b7c3d8.netlify.app/tags/Deep-Learning/">Deep Learning</a></li>
      <li><a href="https://lustrous-paprenjak-b7c3d8.netlify.app/tags/Advanced-Topic/">Advanced Topic</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://lustrous-paprenjak-b7c3d8.netlify.app/">Data Driven Discovery - D3</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>

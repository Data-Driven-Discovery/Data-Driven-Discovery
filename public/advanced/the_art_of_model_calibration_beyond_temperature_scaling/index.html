<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>The Art of Model Calibration: Beyond Temperature Scaling | Data Driven Discovery - D3</title>
<meta name="keywords" content="Machine Learning, Model Calibration, Advanced Topic">
<meta name="description" content="The Art of Model Calibration: Beyond Temperature Scaling In the rapidly evolving field of machine learning, model calibration stands out as a crucial technique, especially when making decisions based on model predictions in high-stakes scenarios like healthcare, finance, and autonomous driving. A well-calibrated model ensures that the predicted probabilities of outcomes reflect their true likeliness, enabling more reliable and interpretable decision-making processes. While temperature scaling is a popular method for calibrating models, this article delves into more advanced strategies, offering valuable insights for both beginners and seasoned practitioners striving to enhance their model&rsquo;s reliability further.">
<meta name="author" content="">
<link rel="canonical" href="https://lustrous-paprenjak-b7c3d8.netlify.app/advanced/The_Art_of_Model_Calibration_Beyond_Temperature_Scaling/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://lustrous-paprenjak-b7c3d8.netlify.app/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://lustrous-paprenjak-b7c3d8.netlify.app/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://lustrous-paprenjak-b7c3d8.netlify.app/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://lustrous-paprenjak-b7c3d8.netlify.app/apple-touch-icon.png">
<link rel="mask-icon" href="https://lustrous-paprenjak-b7c3d8.netlify.app/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="The Art of Model Calibration: Beyond Temperature Scaling" />
<meta property="og:description" content="The Art of Model Calibration: Beyond Temperature Scaling In the rapidly evolving field of machine learning, model calibration stands out as a crucial technique, especially when making decisions based on model predictions in high-stakes scenarios like healthcare, finance, and autonomous driving. A well-calibrated model ensures that the predicted probabilities of outcomes reflect their true likeliness, enabling more reliable and interpretable decision-making processes. While temperature scaling is a popular method for calibrating models, this article delves into more advanced strategies, offering valuable insights for both beginners and seasoned practitioners striving to enhance their model&rsquo;s reliability further." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://lustrous-paprenjak-b7c3d8.netlify.app/advanced/The_Art_of_Model_Calibration_Beyond_Temperature_Scaling/" /><meta property="article:section" content="advanced" />
<meta property="article:published_time" content="2024-02-05T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-02-05T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="The Art of Model Calibration: Beyond Temperature Scaling"/>
<meta name="twitter:description" content="The Art of Model Calibration: Beyond Temperature Scaling In the rapidly evolving field of machine learning, model calibration stands out as a crucial technique, especially when making decisions based on model predictions in high-stakes scenarios like healthcare, finance, and autonomous driving. A well-calibrated model ensures that the predicted probabilities of outcomes reflect their true likeliness, enabling more reliable and interpretable decision-making processes. While temperature scaling is a popular method for calibrating models, this article delves into more advanced strategies, offering valuable insights for both beginners and seasoned practitioners striving to enhance their model&rsquo;s reliability further."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Advanceds",
      "item": "https://lustrous-paprenjak-b7c3d8.netlify.app/advanced/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "The Art of Model Calibration: Beyond Temperature Scaling",
      "item": "https://lustrous-paprenjak-b7c3d8.netlify.app/advanced/The_Art_of_Model_Calibration_Beyond_Temperature_Scaling/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "The Art of Model Calibration: Beyond Temperature Scaling",
  "name": "The Art of Model Calibration: Beyond Temperature Scaling",
  "description": "The Art of Model Calibration: Beyond Temperature Scaling In the rapidly evolving field of machine learning, model calibration stands out as a crucial technique, especially when making decisions based on model predictions in high-stakes scenarios like healthcare, finance, and autonomous driving. A well-calibrated model ensures that the predicted probabilities of outcomes reflect their true likeliness, enabling more reliable and interpretable decision-making processes. While temperature scaling is a popular method for calibrating models, this article delves into more advanced strategies, offering valuable insights for both beginners and seasoned practitioners striving to enhance their model\u0026rsquo;s reliability further.",
  "keywords": [
    "Machine Learning", "Model Calibration", "Advanced Topic"
  ],
  "articleBody": "The Art of Model Calibration: Beyond Temperature Scaling In the rapidly evolving field of machine learning, model calibration stands out as a crucial technique, especially when making decisions based on model predictions in high-stakes scenarios like healthcare, finance, and autonomous driving. A well-calibrated model ensures that the predicted probabilities of outcomes reflect their true likeliness, enabling more reliable and interpretable decision-making processes. While temperature scaling is a popular method for calibrating models, this article delves into more advanced strategies, offering valuable insights for both beginners and seasoned practitioners striving to enhance their model’s reliability further.\nIntroduction Model calibration is often an overlooked aspect of the model development process, yet it’s fundamental in ensuring that the probabilistic predictions reflect real-world probabilities. For instance, in a binary classification problem, if a model predicts a class with a probability of 0.7, ideally, 70% of predictions with that probability should indeed belong to the predicted class. Temperature scaling, a simple yet effective method, involves adjusting the softmax output of a model using a single parameter. However, this technique may not suffice for all models or problems.\nIn this exploration, we venture beyond temperature scaling, discussing and demonstrating alternative calibration methods including Isotonic Regression and Platt Scaling, and explaining when and why to use them. We will also touch upon evaluation metrics like the Brier score and ECE (Expected Calibration Error) to gauge the effectiveness of our calibration efforts.\nGoing Beyond Temperature Scaling Before diving into alternative methods, ensure you have the fundamental libraries installed:\npip install scikit-learn numpy matplotlib Isotonic Regression Isotonic Regression is a non-parametric approach that fits a non-decreasing function to the model scores, effectively calibrating them. It’s particularly useful for correcting the miscalibration in more complex or overfitted models.\nfrom sklearn.isotonic import IsotonicRegression from sklearn.model_selection import train_test_split from sklearn.datasets import make_classification from sklearn.linear_model import LogisticRegression from sklearn.calibration import calibration_curve, CalibratedClassifierCV # Generate synthetic dataset for a binary classification task X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42) # Splitting the dataset into training, validation, and test sets X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.5, random_state=42) X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42) # Train a simple Logistic Regression model model = LogisticRegression(max_iter=1000) model.fit(X_train, y_train) # Calibrating the model with Isotonic Regression isotonic = CalibratedClassifierCV(model, method='isotonic', cv='prefit') isotonic.fit(X_val, y_val) # Predict probabilities on the test set prob_pos_isotonic = isotonic.predict_proba(X_test)[:, 1] # Calibration curve fraction_of_positives, mean_predicted_value = calibration_curve(y_test, prob_pos_isotonic, n_bins=10) # Plotting the calibration curve import matplotlib.pyplot as plt plt.plot(mean_predicted_value, fraction_of_positives, \"s-\", label=\"Isotonic Calibration\") plt.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\") plt.ylabel(\"Fraction of positives\") plt.xlabel(\"Mean predicted value\") plt.legend() plt.show() This code snippet demonstrates how to apply Isotonic Regression calibration to a simple logistic regression model, followed by plotting the calibration curve, which ideally should align closely with the “Perfectly calibrated” line.\nPlatt Scaling Platt Scaling, or logistic calibration, is another approach where a logistic regression model is trained on the decision function’s output for binary classification problems. It’s particularly effective for models that output scores not interpretable as probabilities.\n# Calibrating the model with Platt Scaling platt = CalibratedClassifierCV(model, method='sigmoid', cv='prefit') platt.fit(X_val, y_val) # Predict probabilities on the test set prob_pos_platt = platt.predict_proba(X_test)[:, 1] # Calibration curve fraction_of_positives, mean_predicted_value = calibration_curve(y_test, prob_pos_platt, n_bins=10) plt.plot(mean_predicted_value, fraction_of_positives, \"s-\", label=\"Platt Scaling\") plt.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\") plt.ylabel(\"Fraction of positives\") plt.xlabel(\"Mean predicted value\") plt.legend() plt.show() Platt Scaling addresses model calibration by treating it as a logistic regression problem, adapting well to various types of models, especially SVMs.\nEvaluation of Calibration After calibrating a model, it’s vital to evaluate the calibration’s effectiveness. Two popular metrics are the Brier score for the overall model performance, including calibration, and the Expected Calibration Error (ECE), which specifically measures calibration quality.\nfrom sklearn.metrics import brier_score_loss # Brier score for the initial model initial_probs = model.predict_proba(X_test)[:, 1] brier_initial = brier_score_loss(y_test, initial_probs) # Brier score after Isotonic calibration brier_isotonic = brier_score_loss(y_test, prob_pos_isotonic) print(f\"Brier score (Initial Model): {brier_initial:.4f}\") print(f\"Brier score (Isotonic): {brier_isotonic:.4f}\") Expected Calibration Error can be computed manually or using third-party libraries, providing a scalar value representing the average absolute difference between predicted probabilities and actual outcomes.\nConclusion While temperature scaling offers a straightforward methodology for model calibration, exploring alternatives like Isotonic Regression and Platt Scaling can yield better-calibrated models for certain datasets or model complexities. Calibration techniques enable models to produce probabilities that more accurately reflect reality, enhancing trust in model predictions. Incorporating calibration into your machine learning workflow is not just about improving model performance but also about ensuring reliability and interpretability of the predictions, which is paramount in critical applications.\nAs we’ve ventured beyond temperature scaling, it’s clear that model calibration is both an art and a science requiring careful consideration of the model, problem domain, and available techniques. The choice of calibration method, along with thorough evaluation, can significantly impact the practical usefulness of machine learning models, paving the way for more informed decision-making processes across various fields.\n",
  "wordCount" : "812",
  "inLanguage": "en",
  "datePublished": "2024-02-05T00:00:00Z",
  "dateModified": "2024-02-05T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://lustrous-paprenjak-b7c3d8.netlify.app/advanced/The_Art_of_Model_Calibration_Beyond_Temperature_Scaling/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Data Driven Discovery - D3",
    "logo": {
      "@type": "ImageObject",
      "url": "https://lustrous-paprenjak-b7c3d8.netlify.app/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://lustrous-paprenjak-b7c3d8.netlify.app/" accesskey="h" title="Data Driven Discovery - D3 (Alt + H)">Data Driven Discovery - D3</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      The Art of Model Calibration: Beyond Temperature Scaling
    </h1>
    <div class="post-meta"><span title='2024-02-05 00:00:00 +0000 UTC'>February 5, 2024</span>

</div>
  </header> 
  <div class="post-content"><h1 id="the-art-of-model-calibration-beyond-temperature-scaling">The Art of Model Calibration: Beyond Temperature Scaling<a hidden class="anchor" aria-hidden="true" href="#the-art-of-model-calibration-beyond-temperature-scaling">#</a></h1>
<p>In the rapidly evolving field of machine learning, model calibration stands out as a crucial technique, especially when making decisions based on model predictions in high-stakes scenarios like healthcare, finance, and autonomous driving. A well-calibrated model ensures that the predicted probabilities of outcomes reflect their true likeliness, enabling more reliable and interpretable decision-making processes. While temperature scaling is a popular method for calibrating models, this article delves into more advanced strategies, offering valuable insights for both beginners and seasoned practitioners striving to enhance their model&rsquo;s reliability further.</p>
<h2 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h2>
<p>Model calibration is often an overlooked aspect of the model development process, yet it&rsquo;s fundamental in ensuring that the probabilistic predictions reflect real-world probabilities. For instance, in a binary classification problem, if a model predicts a class with a probability of 0.7, ideally, 70% of predictions with that probability should indeed belong to the predicted class. Temperature scaling, a simple yet effective method, involves adjusting the softmax output of a model using a single parameter. However, this technique may not suffice for all models or problems.</p>
<p>In this exploration, we venture beyond temperature scaling, discussing and demonstrating alternative calibration methods including Isotonic Regression and Platt Scaling, and explaining when and why to use them. We will also touch upon evaluation metrics like the Brier score and ECE (Expected Calibration Error) to gauge the effectiveness of our calibration efforts.</p>
<h2 id="going-beyond-temperature-scaling">Going Beyond Temperature Scaling<a hidden class="anchor" aria-hidden="true" href="#going-beyond-temperature-scaling">#</a></h2>
<p>Before diving into alternative methods, ensure you have the fundamental libraries installed:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>pip install scikit-learn numpy matplotlib
</span></span></code></pre></div><h3 id="isotonic-regression">Isotonic Regression<a hidden class="anchor" aria-hidden="true" href="#isotonic-regression">#</a></h3>
<p>Isotonic Regression is a non-parametric approach that fits a non-decreasing function to the model scores, effectively calibrating them. It&rsquo;s particularly useful for correcting the miscalibration in more complex or overfitted models.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.isotonic <span style="color:#f92672">import</span> IsotonicRegression
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.model_selection <span style="color:#f92672">import</span> train_test_split
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.datasets <span style="color:#f92672">import</span> make_classification
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.linear_model <span style="color:#f92672">import</span> LogisticRegression
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.calibration <span style="color:#f92672">import</span> calibration_curve, CalibratedClassifierCV
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Generate synthetic dataset for a binary classification task</span>
</span></span><span style="display:flex;"><span>X, y <span style="color:#f92672">=</span> make_classification(n_samples<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>, n_features<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>, n_classes<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">42</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Splitting the dataset into training, validation, and test sets</span>
</span></span><span style="display:flex;"><span>X_train, X_temp, y_train, y_temp <span style="color:#f92672">=</span> train_test_split(X, y, test_size<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">42</span>)
</span></span><span style="display:flex;"><span>X_val, X_test, y_val, y_test <span style="color:#f92672">=</span> train_test_split(X_temp, y_temp, test_size<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">42</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Train a simple Logistic Regression model</span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> LogisticRegression(max_iter<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>)
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>fit(X_train, y_train)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Calibrating the model with Isotonic Regression</span>
</span></span><span style="display:flex;"><span>isotonic <span style="color:#f92672">=</span> CalibratedClassifierCV(model, method<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;isotonic&#39;</span>, cv<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;prefit&#39;</span>)
</span></span><span style="display:flex;"><span>isotonic<span style="color:#f92672">.</span>fit(X_val, y_val)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Predict probabilities on the test set</span>
</span></span><span style="display:flex;"><span>prob_pos_isotonic <span style="color:#f92672">=</span> isotonic<span style="color:#f92672">.</span>predict_proba(X_test)[:, <span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Calibration curve</span>
</span></span><span style="display:flex;"><span>fraction_of_positives, mean_predicted_value <span style="color:#f92672">=</span> calibration_curve(y_test, prob_pos_isotonic, n_bins<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Plotting the calibration curve</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>plot(mean_predicted_value, fraction_of_positives, <span style="color:#e6db74">&#34;s-&#34;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Isotonic Calibration&#34;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>plot([<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>], [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>], <span style="color:#e6db74">&#34;k:&#34;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Perfectly calibrated&#34;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#34;Fraction of positives&#34;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#34;Mean predicted value&#34;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>legend()
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><p>This code snippet demonstrates how to apply Isotonic Regression calibration to a simple logistic regression model, followed by plotting the calibration curve, which ideally should align closely with the &ldquo;Perfectly calibrated&rdquo; line.</p>
<h3 id="platt-scaling">Platt Scaling<a hidden class="anchor" aria-hidden="true" href="#platt-scaling">#</a></h3>
<p>Platt Scaling, or logistic calibration, is another approach where a logistic regression model is trained on the decision function&rsquo;s output for binary classification problems. It&rsquo;s particularly effective for models that output scores not interpretable as probabilities.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Calibrating the model with Platt Scaling</span>
</span></span><span style="display:flex;"><span>platt <span style="color:#f92672">=</span> CalibratedClassifierCV(model, method<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;sigmoid&#39;</span>, cv<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;prefit&#39;</span>)
</span></span><span style="display:flex;"><span>platt<span style="color:#f92672">.</span>fit(X_val, y_val)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Predict probabilities on the test set</span>
</span></span><span style="display:flex;"><span>prob_pos_platt <span style="color:#f92672">=</span> platt<span style="color:#f92672">.</span>predict_proba(X_test)[:, <span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Calibration curve</span>
</span></span><span style="display:flex;"><span>fraction_of_positives, mean_predicted_value <span style="color:#f92672">=</span> calibration_curve(y_test, prob_pos_platt, n_bins<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>plot(mean_predicted_value, fraction_of_positives, <span style="color:#e6db74">&#34;s-&#34;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Platt Scaling&#34;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>plot([<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>], [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>], <span style="color:#e6db74">&#34;k:&#34;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Perfectly calibrated&#34;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#34;Fraction of positives&#34;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#34;Mean predicted value&#34;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>legend()
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><p>Platt Scaling addresses model calibration by treating it as a logistic regression problem, adapting well to various types of models, especially SVMs.</p>
<h2 id="evaluation-of-calibration">Evaluation of Calibration<a hidden class="anchor" aria-hidden="true" href="#evaluation-of-calibration">#</a></h2>
<p>After calibrating a model, it&rsquo;s vital to evaluate the calibration&rsquo;s effectiveness. Two popular metrics are the <strong>Brier score</strong> for the overall model performance, including calibration, and the <strong>Expected Calibration Error (ECE)</strong>, which specifically measures calibration quality.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.metrics <span style="color:#f92672">import</span> brier_score_loss
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Brier score for the initial model</span>
</span></span><span style="display:flex;"><span>initial_probs <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>predict_proba(X_test)[:, <span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>brier_initial <span style="color:#f92672">=</span> brier_score_loss(y_test, initial_probs)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Brier score after Isotonic calibration</span>
</span></span><span style="display:flex;"><span>brier_isotonic <span style="color:#f92672">=</span> brier_score_loss(y_test, prob_pos_isotonic)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Brier score (Initial Model): </span><span style="color:#e6db74">{</span>brier_initial<span style="color:#e6db74">:</span><span style="color:#e6db74">.4f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Brier score (Isotonic): </span><span style="color:#e6db74">{</span>brier_isotonic<span style="color:#e6db74">:</span><span style="color:#e6db74">.4f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><p>Expected Calibration Error can be computed manually or using third-party libraries, providing a scalar value representing the average absolute difference between predicted probabilities and actual outcomes.</p>
<h2 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h2>
<p>While temperature scaling offers a straightforward methodology for model calibration, exploring alternatives like Isotonic Regression and Platt Scaling can yield better-calibrated models for certain datasets or model complexities. Calibration techniques enable models to produce probabilities that more accurately reflect reality, enhancing trust in model predictions. Incorporating calibration into your machine learning workflow is not just about improving model performance but also about ensuring reliability and interpretability of the predictions, which is paramount in critical applications.</p>
<p>As we&rsquo;ve ventured beyond temperature scaling, it&rsquo;s clear that model calibration is both an art and a science requiring careful consideration of the model, problem domain, and available techniques. The choice of calibration method, along with thorough evaluation, can significantly impact the practical usefulness of machine learning models, paving the way for more informed decision-making processes across various fields.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://lustrous-paprenjak-b7c3d8.netlify.app/tags/Machine-Learning/">Machine Learning</a></li>
      <li><a href="https://lustrous-paprenjak-b7c3d8.netlify.app/tags/Model-Calibration/">Model Calibration</a></li>
      <li><a href="https://lustrous-paprenjak-b7c3d8.netlify.app/tags/Advanced-Topic/">Advanced Topic</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://lustrous-paprenjak-b7c3d8.netlify.app/">Data Driven Discovery - D3</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>

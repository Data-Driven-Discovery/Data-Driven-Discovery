<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Data Driven Discovery - D3</title>
<meta name="keywords" content="">
<meta name="description" content="Deep Dive into Model Distillation: Strategies for Optimizing Large-Scale Models In the rapidly advancing field of machine learning, deploying large-scale models efficiently remains a critical challenge. While these models, like those based on deep neural networks, have shown remarkable accuracy and capabilities, their size often makes them impractical for certain applications, especially those with limited computational resources. This is where model distillation comes into play. In this article, we will explore what model distillation is, why it’s important, and how you can implement it to optimize your large-scale models.">
<meta name="author" content="">
<link rel="canonical" href="http://example.org/advanced/Deep_Dive_into_Model_Distillation_Strategies_for_Optimizing_Large-Scale_Models/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://example.org/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://example.org/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://example.org/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://example.org/apple-touch-icon.png">
<link rel="mask-icon" href="http://example.org/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="" />
<meta property="og:description" content="Deep Dive into Model Distillation: Strategies for Optimizing Large-Scale Models In the rapidly advancing field of machine learning, deploying large-scale models efficiently remains a critical challenge. While these models, like those based on deep neural networks, have shown remarkable accuracy and capabilities, their size often makes them impractical for certain applications, especially those with limited computational resources. This is where model distillation comes into play. In this article, we will explore what model distillation is, why it’s important, and how you can implement it to optimize your large-scale models." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://example.org/advanced/Deep_Dive_into_Model_Distillation_Strategies_for_Optimizing_Large-Scale_Models/" /><meta property="article:section" content="advanced" />



<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content=""/>
<meta name="twitter:description" content="Deep Dive into Model Distillation: Strategies for Optimizing Large-Scale Models In the rapidly advancing field of machine learning, deploying large-scale models efficiently remains a critical challenge. While these models, like those based on deep neural networks, have shown remarkable accuracy and capabilities, their size often makes them impractical for certain applications, especially those with limited computational resources. This is where model distillation comes into play. In this article, we will explore what model distillation is, why it’s important, and how you can implement it to optimize your large-scale models."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Advanceds",
      "item": "http://example.org/advanced/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "",
      "item": "http://example.org/advanced/Deep_Dive_into_Model_Distillation_Strategies_for_Optimizing_Large-Scale_Models/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "",
  "name": "",
  "description": "Deep Dive into Model Distillation: Strategies for Optimizing Large-Scale Models In the rapidly advancing field of machine learning, deploying large-scale models efficiently remains a critical challenge. While these models, like those based on deep neural networks, have shown remarkable accuracy and capabilities, their size often makes them impractical for certain applications, especially those with limited computational resources. This is where model distillation comes into play. In this article, we will explore what model distillation is, why it’s important, and how you can implement it to optimize your large-scale models.",
  "keywords": [
    
  ],
  "articleBody": "Deep Dive into Model Distillation: Strategies for Optimizing Large-Scale Models In the rapidly advancing field of machine learning, deploying large-scale models efficiently remains a critical challenge. While these models, like those based on deep neural networks, have shown remarkable accuracy and capabilities, their size often makes them impractical for certain applications, especially those with limited computational resources. This is where model distillation comes into play. In this article, we will explore what model distillation is, why it’s important, and how you can implement it to optimize your large-scale models. Whether you’re a beginner or have some experience in machine learning, this guide aims to offer valuable insights and advanced tips on model distillation.\nIntroduction to Model Distillation Model distillation is a technique that involves training a smaller, simpler model (referred to as the student model) to replicate the behavior of a larger, more complex model (known as the teacher model) or an ensemble of models. The core idea is to transfer the knowledge from the cumbersome model to a more compact and efficient one without a significant loss in performance. This process not only reduces the computational resources needed but also allows for the deployment of these models on devices with limited processing capabilities, such as mobile phones and IoT devices.\nWhy Model Distillation Matters Scalability: As models grow in complexity and size, deploying them at scale becomes challenging. Model distillation allows for the creation of smaller models that can be easily scaled. Efficiency: Smaller models require less computational power, reducing both the costs and the carbon footprint associated with running large-scale models. Accessibility: By enabling the deployment of powerful models on edge devices, model distillation broadens the applicability and accessibility of advanced machine learning models. The Process of Model Distillation The general process of model distillation involves several key steps:\nTraining the Teacher Model: This is the initial step where the complex, larger model is trained to achieve high performance on the task at hand. Generating Soft Labels: The teacher model is used to make predictions on a dataset, creating soft labels that capture the predicted probabilities of each class. Training the Student Model: The student model is then trained not only on the original dataset but also to mimic the soft labels generated by the teacher model, effectively learning from the teacher’s outputs. Implementing Model Distillation Let’s dive into a simple example using Python to illustrate how model distillation can be implemented. For this example, we will use TensorFlow, a popular machine learning library.\nSetting up the Environment First, ensure you have TensorFlow installed. If not, you can install it using pip:\npip install tensorflow Training the Teacher Model For demonstration purposes, we will use a pre-trained model from TensorFlow’s model library as our teacher model. However, in a real-world scenario, you would train this model on your specific dataset.\nimport tensorflow as tf # Load a pre-trained model as our teacher teacher_model = tf.keras.applications.MobileNetV2(input_shape=(224, 224, 3), include_top=True, weights='imagenet') teacher_model.summary() Generating Soft Labels We will then use the teacher model to generate soft labels on a new dataset (which, for simplicity in this example, will be a small subset of data).\nimport numpy as np # Assuming X_test is our dataset and has been preprocessed # Generate soft labels soft_labels = teacher_model.predict(X_test) Training the Student Model Now, we will define a simpler model as our student. The student’s architecture doesn’t have to match the teacher’s; it just needs to be capable of learning from the soft labels.\n# Define a simpler model student_model = tf.keras.models.Sequential([ tf.keras.layers.Conv2D(16, 3, activation='relu', input_shape=(224, 224, 3)), tf.keras.layers.MaxPooling2D(), tf.keras.layers.Flatten(), tf.keras.layers.Dense(10, activation='softmax') # Assuming 10 classes ]) # Compile the model student_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) # Train the student model # Note: Your dataset needs to be adjusted to match the input size of the student model student_model.fit(X_train, soft_labels, epochs=10) Here, X_train and X_test need to be prepared datasets that match the input requirements of the models used.\nConclusion Model distillation presents a viable path for deploying large, powerful machine learning models in resource-constrained environments. By training a smaller model to emulate the functionality of a larger one, we can achieve a balance between performance and efficiency. This technique opens up new possibilities for applying advanced AI in areas previously thought impractical due to resource limitations. Remember, the key to successful model distillation lies in careful selection and training of both the teacher and student models, a process that may require experimentation to optimize for your specific needs.\nIncorporating model distillation into your machine learning pipeline can significantly enhance the scalability and efficiency of your AI applications. As we’ve seen, the process can be relatively straightforward but requires a solid understanding of both your models and the tasks at hand. By mastering model distillation, you can push the boundaries of where and how AI can be applied, bringing sophisticated solutions to a broader range of devices and platforms.\n",
  "wordCount" : "812",
  "inLanguage": "en",
  "datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://example.org/advanced/Deep_Dive_into_Model_Distillation_Strategies_for_Optimizing_Large-Scale_Models/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Data Driven Discovery - D3",
    "logo": {
      "@type": "ImageObject",
      "url": "http://example.org/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://example.org/" accesskey="h" title="Data Driven Discovery - D3 (Alt + H)">Data Driven Discovery - D3</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      
    </h1>
    <div class="post-meta">

</div>
  </header> 
  <div class="post-content"><h1 id="deep-dive-into-model-distillation-strategies-for-optimizing-large-scale-models">Deep Dive into Model Distillation: Strategies for Optimizing Large-Scale Models<a hidden class="anchor" aria-hidden="true" href="#deep-dive-into-model-distillation-strategies-for-optimizing-large-scale-models">#</a></h1>
<p>In the rapidly advancing field of machine learning, deploying large-scale models efficiently remains a critical challenge. While these models, like those based on deep neural networks, have shown remarkable accuracy and capabilities, their size often makes them impractical for certain applications, especially those with limited computational resources. This is where model distillation comes into play. In this article, we will explore what model distillation is, why it’s important, and how you can implement it to optimize your large-scale models. Whether you&rsquo;re a beginner or have some experience in machine learning, this guide aims to offer valuable insights and advanced tips on model distillation.</p>
<h2 id="introduction-to-model-distillation">Introduction to Model Distillation<a hidden class="anchor" aria-hidden="true" href="#introduction-to-model-distillation">#</a></h2>
<p>Model distillation is a technique that involves training a smaller, simpler model (referred to as the student model) to replicate the behavior of a larger, more complex model (known as the teacher model) or an ensemble of models. The core idea is to transfer the knowledge from the cumbersome model to a more compact and efficient one without a significant loss in performance. This process not only reduces the computational resources needed but also allows for the deployment of these models on devices with limited processing capabilities, such as mobile phones and IoT devices.</p>
<h2 id="why-model-distillation-matters">Why Model Distillation Matters<a hidden class="anchor" aria-hidden="true" href="#why-model-distillation-matters">#</a></h2>
<ul>
<li><strong>Scalability</strong>: As models grow in complexity and size, deploying them at scale becomes challenging. Model distillation allows for the creation of smaller models that can be easily scaled.</li>
<li><strong>Efficiency</strong>: Smaller models require less computational power, reducing both the costs and the carbon footprint associated with running large-scale models.</li>
<li><strong>Accessibility</strong>: By enabling the deployment of powerful models on edge devices, model distillation broadens the applicability and accessibility of advanced machine learning models.</li>
</ul>
<h2 id="the-process-of-model-distillation">The Process of Model Distillation<a hidden class="anchor" aria-hidden="true" href="#the-process-of-model-distillation">#</a></h2>
<p>The general process of model distillation involves several key steps:</p>
<ol>
<li><strong>Training the Teacher Model</strong>: This is the initial step where the complex, larger model is trained to achieve high performance on the task at hand.</li>
<li><strong>Generating Soft Labels</strong>: The teacher model is used to make predictions on a dataset, creating soft labels that capture the predicted probabilities of each class.</li>
<li><strong>Training the Student Model</strong>: The student model is then trained not only on the original dataset but also to mimic the soft labels generated by the teacher model, effectively learning from the teacher’s outputs.</li>
</ol>
<h2 id="implementing-model-distillation">Implementing Model Distillation<a hidden class="anchor" aria-hidden="true" href="#implementing-model-distillation">#</a></h2>
<p>Let&rsquo;s dive into a simple example using Python to illustrate how model distillation can be implemented. For this example, we will use TensorFlow, a popular machine learning library.</p>
<h3 id="setting-up-the-environment">Setting up the Environment<a hidden class="anchor" aria-hidden="true" href="#setting-up-the-environment">#</a></h3>
<p>First, ensure you have TensorFlow installed. If not, you can install it using pip:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>pip install tensorflow
</span></span></code></pre></div><h3 id="training-the-teacher-model">Training the Teacher Model<a hidden class="anchor" aria-hidden="true" href="#training-the-teacher-model">#</a></h3>
<p>For demonstration purposes, we will use a pre-trained model from TensorFlow&rsquo;s model library as our teacher model. However, in a real-world scenario, you would train this model on your specific dataset.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> tensorflow <span style="color:#66d9ef">as</span> tf
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Load a pre-trained model as our teacher</span>
</span></span><span style="display:flex;"><span>teacher_model <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>applications<span style="color:#f92672">.</span>MobileNetV2(input_shape<span style="color:#f92672">=</span>(<span style="color:#ae81ff">224</span>, <span style="color:#ae81ff">224</span>, <span style="color:#ae81ff">3</span>),
</span></span><span style="display:flex;"><span>                                                  include_top<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>                                                  weights<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;imagenet&#39;</span>)
</span></span><span style="display:flex;"><span>teacher_model<span style="color:#f92672">.</span>summary()
</span></span></code></pre></div><h3 id="generating-soft-labels">Generating Soft Labels<a hidden class="anchor" aria-hidden="true" href="#generating-soft-labels">#</a></h3>
<p>We will then use the teacher model to generate soft labels on a new dataset (which, for simplicity in this example, will be a small subset of data).</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Assuming X_test is our dataset and has been preprocessed</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Generate soft labels</span>
</span></span><span style="display:flex;"><span>soft_labels <span style="color:#f92672">=</span> teacher_model<span style="color:#f92672">.</span>predict(X_test)
</span></span></code></pre></div><h3 id="training-the-student-model">Training the Student Model<a hidden class="anchor" aria-hidden="true" href="#training-the-student-model">#</a></h3>
<p>Now, we will define a simpler model as our student. The student&rsquo;s architecture doesn’t have to match the teacher&rsquo;s; it just needs to be capable of learning from the soft labels.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Define a simpler model</span>
</span></span><span style="display:flex;"><span>student_model <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>models<span style="color:#f92672">.</span>Sequential([
</span></span><span style="display:flex;"><span>  tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Conv2D(<span style="color:#ae81ff">16</span>, <span style="color:#ae81ff">3</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>, input_shape<span style="color:#f92672">=</span>(<span style="color:#ae81ff">224</span>, <span style="color:#ae81ff">224</span>, <span style="color:#ae81ff">3</span>)),
</span></span><span style="display:flex;"><span>  tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>MaxPooling2D(),
</span></span><span style="display:flex;"><span>  tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Flatten(),
</span></span><span style="display:flex;"><span>  tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(<span style="color:#ae81ff">10</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;softmax&#39;</span>)  <span style="color:#75715e"># Assuming 10 classes</span>
</span></span><span style="display:flex;"><span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Compile the model</span>
</span></span><span style="display:flex;"><span>student_model<span style="color:#f92672">.</span>compile(optimizer<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;adam&#39;</span>,
</span></span><span style="display:flex;"><span>                      loss<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;categorical_crossentropy&#39;</span>,
</span></span><span style="display:flex;"><span>                      metrics<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;accuracy&#39;</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Train the student model</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Note: Your dataset needs to be adjusted to match the input size of the student model</span>
</span></span><span style="display:flex;"><span>student_model<span style="color:#f92672">.</span>fit(X_train, soft_labels, epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>)
</span></span></code></pre></div><p>Here, <code>X_train</code> and <code>X_test</code> need to be prepared datasets that match the input requirements of the models used.</p>
<h3 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h3>
<p>Model distillation presents a viable path for deploying large, powerful machine learning models in resource-constrained environments. By training a smaller model to emulate the functionality of a larger one, we can achieve a balance between performance and efficiency. This technique opens up new possibilities for applying advanced AI in areas previously thought impractical due to resource limitations. Remember, the key to successful model distillation lies in careful selection and training of both the teacher and student models, a process that may require experimentation to optimize for your specific needs.</p>
<p>Incorporating model distillation into your machine learning pipeline can significantly enhance the scalability and efficiency of your AI applications. As we&rsquo;ve seen, the process can be relatively straightforward but requires a solid understanding of both your models and the tasks at hand. By mastering model distillation, you can push the boundaries of where and how AI can be applied, bringing sophisticated solutions to a broader range of devices and platforms.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="http://example.org/">Data Driven Discovery - D3</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>

<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Data Driven Discovery - D3</title>
<meta name="keywords" content="">
<meta name="description" content="Optimal Resource Allocation in Distributed Machine Learning In the burgeoning field of artificial intelligence (AI), distributed machine learning (ML) stands out as a pivotal method for tackling complex computational tasks. This method leverages the power of multiple computing units to process data more efficiently, thereby accelerating the training of models on large datasets. However, to harness its full potential, it&rsquo;s crucial to address the challenge of optimal resource allocation. This article delves into effective strategies and techniques to maximize resource utilization in distributed ML environments, catering to both beginners and advanced practitioners.">
<meta name="author" content="">
<link rel="canonical" href="http://example.org/advanced/Optimal_Resource_Allocation_in_Distributed_Machine_Learning/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://example.org/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://example.org/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://example.org/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://example.org/apple-touch-icon.png">
<link rel="mask-icon" href="http://example.org/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="" />
<meta property="og:description" content="Optimal Resource Allocation in Distributed Machine Learning In the burgeoning field of artificial intelligence (AI), distributed machine learning (ML) stands out as a pivotal method for tackling complex computational tasks. This method leverages the power of multiple computing units to process data more efficiently, thereby accelerating the training of models on large datasets. However, to harness its full potential, it&rsquo;s crucial to address the challenge of optimal resource allocation. This article delves into effective strategies and techniques to maximize resource utilization in distributed ML environments, catering to both beginners and advanced practitioners." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://example.org/advanced/Optimal_Resource_Allocation_in_Distributed_Machine_Learning/" /><meta property="article:section" content="advanced" />



<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content=""/>
<meta name="twitter:description" content="Optimal Resource Allocation in Distributed Machine Learning In the burgeoning field of artificial intelligence (AI), distributed machine learning (ML) stands out as a pivotal method for tackling complex computational tasks. This method leverages the power of multiple computing units to process data more efficiently, thereby accelerating the training of models on large datasets. However, to harness its full potential, it&rsquo;s crucial to address the challenge of optimal resource allocation. This article delves into effective strategies and techniques to maximize resource utilization in distributed ML environments, catering to both beginners and advanced practitioners."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Advanceds",
      "item": "http://example.org/advanced/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "",
      "item": "http://example.org/advanced/Optimal_Resource_Allocation_in_Distributed_Machine_Learning/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "",
  "name": "",
  "description": "Optimal Resource Allocation in Distributed Machine Learning In the burgeoning field of artificial intelligence (AI), distributed machine learning (ML) stands out as a pivotal method for tackling complex computational tasks. This method leverages the power of multiple computing units to process data more efficiently, thereby accelerating the training of models on large datasets. However, to harness its full potential, it\u0026rsquo;s crucial to address the challenge of optimal resource allocation. This article delves into effective strategies and techniques to maximize resource utilization in distributed ML environments, catering to both beginners and advanced practitioners.",
  "keywords": [
    
  ],
  "articleBody": "Optimal Resource Allocation in Distributed Machine Learning In the burgeoning field of artificial intelligence (AI), distributed machine learning (ML) stands out as a pivotal method for tackling complex computational tasks. This method leverages the power of multiple computing units to process data more efficiently, thereby accelerating the training of models on large datasets. However, to harness its full potential, it’s crucial to address the challenge of optimal resource allocation. This article delves into effective strategies and techniques to maximize resource utilization in distributed ML environments, catering to both beginners and advanced practitioners.\nIntroduction Distributed Machine Learning is a paradigm that distributes computational tasks across multiple devices or nodes, aimed at reducing training time and handling voluminous data effortlessly. However, as the scale of data and the complexity of models grow, the challenge of optimally allocating resources becomes paramount. The goal is to ensure that the distribution not only speeds up the process but also does so cost-efficiently, making effective resource allocation an essential consideration for anyone looking to scale their machine learning operations.\nOptimizing Resource Allocation: Strategies and Techniques Understanding the Landscape Before diving into allocation strategies, it’s important to understand the components that need to be optimized:\nCompute Resources: Including CPUs, GPUs, and TPUs. Memory: RAM and disk space required by the training process. Network: Bandwidth and latency between nodes. Allocation Techniques 1. Efficient Data Parallelism Data parallelism involves splitting the dataset across multiple processors to perform computations in parallel. To implement this effectively, one can use frameworks such as TensorFlow or PyTorch.\nimport tensorflow as tf strategy = tf.distribute.MirroredStrategy() with strategy.scope(): # Define your model here model = tf.keras.Sequential([...]) model.compile(...) 2. Model Parallelism Different parts of a neural network model are trained on different processors. This technique is particularly useful for models that are too large to fit into a single GPU’s memory.\n# Pseudo code for TensorFlow (conceptual, implement according to specific needs) devices = ['/gpu:0', '/gpu:1'] with tf.device(devices[0]): # Define the first part of the model with tf.device(devices[1]): # Define the second part of the model 3. Hybrid Approaches Combining data and model parallelism can sometimes offer the best of both worlds, particularly for very large-scale training tasks.\n# This is a conceptual illustration. Implement based on the specific framework and requirements. Advanced Techniques Automated Resource Allocation: Utilize machine learning operations (MLOps) tools to dynamically allocate resources based on the workload. Spot Instances and Preemptible VMs: Cloud computing services offer lower-cost computing instances that can significantly reduce training costs. Code Snippet: Dynamic Resource Allocation with Kubernetes # This is a Kubernetes YAML configuration example for auto-scaling apiVersion: autoscaling/v2beta2 kind: HorizontalPodAutoscaler metadata: name: ml-model-autoscaler spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: ml-model-deployment minReplicas: 1 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 80 This Kubernetes configuration automatically increases the number of pods running your ML model deployment when the CPU utilization exceeds 80%, ensuring optimal resource usage without manual intervention.\nConclusion Optimal resource allocation in distributed machine learning is a multifaceted challenge that necessitates a comprehensive understanding of both the computational requirements of ML models and the capabilities of distributed computing environments. By employing strategies like efficient data parallelism, model parallelism, and leveraging cloud computing resources intelligently, practitioners can significantly reduce training times and costs.\nMoreover, embracing automated tools and platforms can alleviate much of the complexity involved in manual resource management. As we continue to advance in the field of machine learning, the importance of strategic resource allocation will only become more pronounced, playing a crucial role in facilitating cutting-edge ML research and development.\nNavigating the landscape of distributed machine learning can be complex, but by focusing on effective resource allocation, organizations can unlock new efficiencies and capabilities, paving the way for innovative applications and technologies. Whether you’re just starting out or looking to refine your approach to distributed ML, embracing these strategies will be key to your success.\n",
  "wordCount" : "648",
  "inLanguage": "en",
  "datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://example.org/advanced/Optimal_Resource_Allocation_in_Distributed_Machine_Learning/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Data Driven Discovery - D3",
    "logo": {
      "@type": "ImageObject",
      "url": "http://example.org/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://example.org/" accesskey="h" title="Data Driven Discovery - D3 (Alt + H)">Data Driven Discovery - D3</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      
    </h1>
    <div class="post-meta">

</div>
  </header> 
  <div class="post-content"><h1 id="optimal-resource-allocation-in-distributed-machine-learning">Optimal Resource Allocation in Distributed Machine Learning<a hidden class="anchor" aria-hidden="true" href="#optimal-resource-allocation-in-distributed-machine-learning">#</a></h1>
<p>In the burgeoning field of artificial intelligence (AI), distributed machine learning (ML) stands out as a pivotal method for tackling complex computational tasks. This method leverages the power of multiple computing units to process data more efficiently, thereby accelerating the training of models on large datasets. However, to harness its full potential, it&rsquo;s crucial to address the challenge of optimal resource allocation. This article delves into effective strategies and techniques to maximize resource utilization in distributed ML environments, catering to both beginners and advanced practitioners.</p>
<h2 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h2>
<p>Distributed Machine Learning is a paradigm that distributes computational tasks across multiple devices or nodes, aimed at reducing training time and handling voluminous data effortlessly. However, as the scale of data and the complexity of models grow, the challenge of optimally allocating resources becomes paramount. The goal is to ensure that the distribution not only speeds up the process but also does so cost-efficiently, making effective resource allocation an essential consideration for anyone looking to scale their machine learning operations.</p>
<h2 id="optimizing-resource-allocation-strategies-and-techniques">Optimizing Resource Allocation: Strategies and Techniques<a hidden class="anchor" aria-hidden="true" href="#optimizing-resource-allocation-strategies-and-techniques">#</a></h2>
<h3 id="understanding-the-landscape">Understanding the Landscape<a hidden class="anchor" aria-hidden="true" href="#understanding-the-landscape">#</a></h3>
<p>Before diving into allocation strategies, it&rsquo;s important to understand the components that need to be optimized:</p>
<ul>
<li><strong>Compute Resources:</strong> Including CPUs, GPUs, and TPUs.</li>
<li><strong>Memory:</strong> RAM and disk space required by the training process.</li>
<li><strong>Network:</strong> Bandwidth and latency between nodes.</li>
</ul>
<h3 id="allocation-techniques">Allocation Techniques<a hidden class="anchor" aria-hidden="true" href="#allocation-techniques">#</a></h3>
<h4 id="1-efficient-data-parallelism">1. Efficient Data Parallelism<a hidden class="anchor" aria-hidden="true" href="#1-efficient-data-parallelism">#</a></h4>
<p>Data parallelism involves splitting the dataset across multiple processors to perform computations in parallel. To implement this effectively, one can use frameworks such as TensorFlow or PyTorch.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> tensorflow <span style="color:#66d9ef">as</span> tf
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>strategy <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>distribute<span style="color:#f92672">.</span>MirroredStrategy()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">with</span> strategy<span style="color:#f92672">.</span>scope():
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># Define your model here</span>
</span></span><span style="display:flex;"><span>  model <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>Sequential([<span style="color:#f92672">...</span>])
</span></span><span style="display:flex;"><span>  model<span style="color:#f92672">.</span>compile(<span style="color:#f92672">...</span>)
</span></span></code></pre></div><h4 id="2-model-parallelism">2. Model Parallelism<a hidden class="anchor" aria-hidden="true" href="#2-model-parallelism">#</a></h4>
<p>Different parts of a neural network model are trained on different processors. This technique is particularly useful for models that are too large to fit into a single GPU&rsquo;s memory.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Pseudo code for TensorFlow (conceptual, implement according to specific needs)</span>
</span></span><span style="display:flex;"><span>devices <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;/gpu:0&#39;</span>, <span style="color:#e6db74">&#39;/gpu:1&#39;</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>device(devices[<span style="color:#ae81ff">0</span>]):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Define the first part of the model</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>device(devices[<span style="color:#ae81ff">1</span>]):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Define the second part of the model</span>
</span></span></code></pre></div><h4 id="3-hybrid-approaches">3. Hybrid Approaches<a hidden class="anchor" aria-hidden="true" href="#3-hybrid-approaches">#</a></h4>
<p>Combining data and model parallelism can sometimes offer the best of both worlds, particularly for very large-scale training tasks.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># This is a conceptual illustration. Implement based on the specific framework and requirements.</span>
</span></span></code></pre></div><h3 id="advanced-techniques">Advanced Techniques<a hidden class="anchor" aria-hidden="true" href="#advanced-techniques">#</a></h3>
<ul>
<li><strong>Automated Resource Allocation</strong>: Utilize machine learning operations (MLOps) tools to dynamically allocate resources based on the workload.</li>
<li><strong>Spot Instances and Preemptible VMs</strong>: Cloud computing services offer lower-cost computing instances that can significantly reduce training costs.</li>
</ul>
<h3 id="code-snippet-dynamic-resource-allocation-with-kubernetes">Code Snippet: Dynamic Resource Allocation with Kubernetes<a hidden class="anchor" aria-hidden="true" href="#code-snippet-dynamic-resource-allocation-with-kubernetes">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#75715e"># This is a Kubernetes YAML configuration example for auto-scaling</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">autoscaling/v2beta2</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">HorizontalPodAutoscaler</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">ml-model-autoscaler</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">scaleTargetRef</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">apps/v1</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Deployment</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">name</span>: <span style="color:#ae81ff">ml-model-deployment</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">minReplicas</span>: <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">maxReplicas</span>: <span style="color:#ae81ff">10</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">metrics</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">type</span>: <span style="color:#ae81ff">Resource</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">resource</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">name</span>: <span style="color:#ae81ff">cpu</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">target</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">type</span>: <span style="color:#ae81ff">Utilization</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">averageUtilization</span>: <span style="color:#ae81ff">80</span>
</span></span></code></pre></div><p>This Kubernetes configuration automatically increases the number of pods running your ML model deployment when the CPU utilization exceeds 80%, ensuring optimal resource usage without manual intervention.</p>
<h2 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h2>
<p>Optimal resource allocation in distributed machine learning is a multifaceted challenge that necessitates a comprehensive understanding of both the computational requirements of ML models and the capabilities of distributed computing environments. By employing strategies like efficient data parallelism, model parallelism, and leveraging cloud computing resources intelligently, practitioners can significantly reduce training times and costs.</p>
<p>Moreover, embracing automated tools and platforms can alleviate much of the complexity involved in manual resource management. As we continue to advance in the field of machine learning, the importance of strategic resource allocation will only become more pronounced, playing a crucial role in facilitating cutting-edge ML research and development.</p>
<p>Navigating the landscape of distributed machine learning can be complex, but by focusing on effective resource allocation, organizations can unlock new efficiencies and capabilities, paving the way for innovative applications and technologies. Whether you&rsquo;re just starting out or looking to refine your approach to distributed ML, embracing these strategies will be key to your success.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="http://example.org/">Data Driven Discovery - D3</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>

<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Reinforcement Learning in Non-Stationary Environments: Overcoming Drift and Shift | Data Driven Discovery - D3</title>
<meta name="keywords" content="Reinforcement Learning, Machine Learning, Advanced Topic">
<meta name="description" content="Reinforcement Learning in Non-Stationary Environments: Overcoming Drift and Shift In the dynamic world of Machine Learning, Reinforcement Learning (RL) stands out for its ability to make decisions and learn from them in real-time. However, when deploying RL models in real-world scenarios, one of the significant challenges is dealing with non-stationary environments. These are settings where the underlying data distribution changes over time, often referred to as concept drift or shift, posing a significant hurdle for maintaining the performance of RL models.">
<meta name="author" content="">
<link rel="canonical" href="https://lustrous-paprenjak-b7c3d8.netlify.app/advanced/Reinforcement_Learning_in_Non-Stationary_Environments_Overcoming_Drift_and_Shift/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://lustrous-paprenjak-b7c3d8.netlify.app/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://lustrous-paprenjak-b7c3d8.netlify.app/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://lustrous-paprenjak-b7c3d8.netlify.app/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://lustrous-paprenjak-b7c3d8.netlify.app/apple-touch-icon.png">
<link rel="mask-icon" href="https://lustrous-paprenjak-b7c3d8.netlify.app/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Reinforcement Learning in Non-Stationary Environments: Overcoming Drift and Shift" />
<meta property="og:description" content="Reinforcement Learning in Non-Stationary Environments: Overcoming Drift and Shift In the dynamic world of Machine Learning, Reinforcement Learning (RL) stands out for its ability to make decisions and learn from them in real-time. However, when deploying RL models in real-world scenarios, one of the significant challenges is dealing with non-stationary environments. These are settings where the underlying data distribution changes over time, often referred to as concept drift or shift, posing a significant hurdle for maintaining the performance of RL models." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://lustrous-paprenjak-b7c3d8.netlify.app/advanced/Reinforcement_Learning_in_Non-Stationary_Environments_Overcoming_Drift_and_Shift/" /><meta property="article:section" content="advanced" />
<meta property="article:published_time" content="2024-02-05T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-02-05T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Reinforcement Learning in Non-Stationary Environments: Overcoming Drift and Shift"/>
<meta name="twitter:description" content="Reinforcement Learning in Non-Stationary Environments: Overcoming Drift and Shift In the dynamic world of Machine Learning, Reinforcement Learning (RL) stands out for its ability to make decisions and learn from them in real-time. However, when deploying RL models in real-world scenarios, one of the significant challenges is dealing with non-stationary environments. These are settings where the underlying data distribution changes over time, often referred to as concept drift or shift, posing a significant hurdle for maintaining the performance of RL models."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Advanceds",
      "item": "https://lustrous-paprenjak-b7c3d8.netlify.app/advanced/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Reinforcement Learning in Non-Stationary Environments: Overcoming Drift and Shift",
      "item": "https://lustrous-paprenjak-b7c3d8.netlify.app/advanced/Reinforcement_Learning_in_Non-Stationary_Environments_Overcoming_Drift_and_Shift/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Reinforcement Learning in Non-Stationary Environments: Overcoming Drift and Shift",
  "name": "Reinforcement Learning in Non-Stationary Environments: Overcoming Drift and Shift",
  "description": "Reinforcement Learning in Non-Stationary Environments: Overcoming Drift and Shift In the dynamic world of Machine Learning, Reinforcement Learning (RL) stands out for its ability to make decisions and learn from them in real-time. However, when deploying RL models in real-world scenarios, one of the significant challenges is dealing with non-stationary environments. These are settings where the underlying data distribution changes over time, often referred to as concept drift or shift, posing a significant hurdle for maintaining the performance of RL models.",
  "keywords": [
    "Reinforcement Learning", "Machine Learning", "Advanced Topic"
  ],
  "articleBody": "Reinforcement Learning in Non-Stationary Environments: Overcoming Drift and Shift In the dynamic world of Machine Learning, Reinforcement Learning (RL) stands out for its ability to make decisions and learn from them in real-time. However, when deploying RL models in real-world scenarios, one of the significant challenges is dealing with non-stationary environments. These are settings where the underlying data distribution changes over time, often referred to as concept drift or shift, posing a significant hurdle for maintaining the performance of RL models. This article delves into strategies and advanced techniques to overcome these challenges, ensuring your models remain robust and effective over time.\nIntroduction Reinforcement Learning has revolutionized areas such as autonomous driving, robotics, and game playing, by enabling models to learn optimal behaviors through trial and error. However, most tutorials and examples assume a stationary environment where the rules of the game don’t change. In contrast, real-world scenarios are far from static. Non-stationary environments are prevalent, from changing market conditions in finance to evolving user preferences in recommendation systems. This article aims to bridge that gap and provide actionable insights into managing RL models in non-stationary environments.\nUnderstanding Non-Stationarity In a non-stationary environment, the transition probabilities and reward functions are not fixed but change over time. This could be gradual or abrupt and can significantly affect the performance of an RL model. Common causes include:\nSeasonal variations influencing user behavior New products or competitors entering the market Regulatory changes affecting operational conditions The key challenge is detecting and adapting to these changes effectively.\nStrategies for Overcoming Drift and Shift 1. Adaptive Learning Rates One simple yet effective method to tackle non-stationarity is to use adaptive learning rates in your RL algorithms. Learning rates determine the step size during the update of model’s weights. Adaptive rates can adjust as the environment evolves.\nExample with TensorFlow: import tensorflow as tf optimizer = tf.keras.optimizers.Adam(learning_rate=0.001) # Assume model is your RL model # Perform a training operation using `optimizer` here 2. Sliding Window Techniques For environments that change gradually, implementing a sliding window approach can help. The idea is to update the model using only the most recent data, discarding older data that may no longer be relevant.\nExample Code Snippet: import numpy as np # Assume `data` is a time-series of experiences or observations window_size = 100 # Define the size of your window current_window = data[-window_size:] # Keeps only the most recent data # Train your model on `current_window` 3. Online Learning and Continuous Adaptation Online learning techniques, where the model is continuously updated with new data, can be particularly effective in non-stationary environments. This allows the model to adapt to changes on-the-fly.\nExample with scikit-learn: from sklearn.linear_model import SGDRegressor # Create the model model = SGDRegressor() # Assume X_train, y_train are your features and labels respectively for x, y in zip(X_train, y_train): model.partial_fit(x.reshape(1, -1), [y]) # Update the model incrementally 4. Change Detection Mechanisms Implementing a mechanism to detect changes in the environment can help trigger adaptation strategies. This can range from simple statistical tests to more complex machine learning models designed to identify shifts in data distribution.\nExample Pseudocode: # Assume `data_stream` is a generator yielding new observations for observation in data_stream: if change_detected(observation): # Trigger model retraining or adaptation pass 5. Multi-Model Approaches Using a portfolio of models, each trained on different segments of the data or under different assumptions, can provide a robust response to changing environments. This approach often leverages ensemble techniques.\nExample Concept: # Assume `models` is a list of different RL models # `data_segment` is the current batch of data to make predictions on predictions = [model.predict(data_segment) for model in models] final_prediction = np.mean(predictions, axis=0) # An example of ensemble prediction Conclusion Adapting reinforcement learning models to non-stationary environments is crucial for real-world applications. This article has introduced several strategies, from adaptive learning rates and sliding window techniques to online learning, change detection, and multi-model approaches. While there is no one-size-fits-all solution, combining these strategies can significantly increase the resilience and adaptability of your RL models. Experimentation and continuous monitoring are key to finding the optimal approach for your specific application, ensuring your models can withstand the test of time and change.\nIn the journey of mastering RL in non-stationary environments, remember that the goal is to build systems capable of learning and evolving as the world changes around them. The future of reinforcement learning is undoubtedly exciting, with endless possibilities for innovation and improvement. Embrace the challenge, and let your models learn not just to play the game but to play it well, no matter how often the rules may change.\n",
  "wordCount" : "767",
  "inLanguage": "en",
  "datePublished": "2024-02-05T00:00:00Z",
  "dateModified": "2024-02-05T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://lustrous-paprenjak-b7c3d8.netlify.app/advanced/Reinforcement_Learning_in_Non-Stationary_Environments_Overcoming_Drift_and_Shift/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Data Driven Discovery - D3",
    "logo": {
      "@type": "ImageObject",
      "url": "https://lustrous-paprenjak-b7c3d8.netlify.app/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://lustrous-paprenjak-b7c3d8.netlify.app/" accesskey="h" title="Data Driven Discovery - D3 (Alt + H)">Data Driven Discovery - D3</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Reinforcement Learning in Non-Stationary Environments: Overcoming Drift and Shift
    </h1>
    <div class="post-meta"><span title='2024-02-05 00:00:00 +0000 UTC'>February 5, 2024</span>

</div>
  </header> 
  <div class="post-content"><h1 id="reinforcement-learning-in-non-stationary-environments-overcoming-drift-and-shift">Reinforcement Learning in Non-Stationary Environments: Overcoming Drift and Shift<a hidden class="anchor" aria-hidden="true" href="#reinforcement-learning-in-non-stationary-environments-overcoming-drift-and-shift">#</a></h1>
<p>In the dynamic world of Machine Learning, Reinforcement Learning (RL) stands out for its ability to make decisions and learn from them in real-time. However, when deploying RL models in real-world scenarios, one of the significant challenges is dealing with non-stationary environments. These are settings where the underlying data distribution changes over time, often referred to as concept drift or shift, posing a significant hurdle for maintaining the performance of RL models. This article delves into strategies and advanced techniques to overcome these challenges, ensuring your models remain robust and effective over time.</p>
<h2 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h2>
<p>Reinforcement Learning has revolutionized areas such as autonomous driving, robotics, and game playing, by enabling models to learn optimal behaviors through trial and error. However, most tutorials and examples assume a stationary environment where the rules of the game don&rsquo;t change. In contrast, real-world scenarios are far from static. Non-stationary environments are prevalent, from changing market conditions in finance to evolving user preferences in recommendation systems. This article aims to bridge that gap and provide actionable insights into managing RL models in non-stationary environments.</p>
<h2 id="understanding-non-stationarity">Understanding Non-Stationarity<a hidden class="anchor" aria-hidden="true" href="#understanding-non-stationarity">#</a></h2>
<p>In a non-stationary environment, the transition probabilities and reward functions are not fixed but change over time. This could be gradual or abrupt and can significantly affect the performance of an RL model. Common causes include:</p>
<ul>
<li>Seasonal variations influencing user behavior</li>
<li>New products or competitors entering the market</li>
<li>Regulatory changes affecting operational conditions</li>
</ul>
<p>The key challenge is detecting and adapting to these changes effectively.</p>
<h2 id="strategies-for-overcoming-drift-and-shift">Strategies for Overcoming Drift and Shift<a hidden class="anchor" aria-hidden="true" href="#strategies-for-overcoming-drift-and-shift">#</a></h2>
<h3 id="1-adaptive-learning-rates">1. Adaptive Learning Rates<a hidden class="anchor" aria-hidden="true" href="#1-adaptive-learning-rates">#</a></h3>
<p>One simple yet effective method to tackle non-stationarity is to use adaptive learning rates in your RL algorithms. Learning rates determine the step size during the update of model&rsquo;s weights. Adaptive rates can adjust as the environment evolves.</p>
<h4 id="example-with-tensorflow">Example with TensorFlow:<a hidden class="anchor" aria-hidden="true" href="#example-with-tensorflow">#</a></h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> tensorflow <span style="color:#66d9ef">as</span> tf
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>optimizer <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>Adam(learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">0.001</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Assume model is your RL model</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Perform a training operation using `optimizer` here</span>
</span></span></code></pre></div><h3 id="2-sliding-window-techniques">2. Sliding Window Techniques<a hidden class="anchor" aria-hidden="true" href="#2-sliding-window-techniques">#</a></h3>
<p>For environments that change gradually, implementing a sliding window approach can help. The idea is to update the model using only the most recent data, discarding older data that may no longer be relevant.</p>
<h4 id="example-code-snippet">Example Code Snippet:<a hidden class="anchor" aria-hidden="true" href="#example-code-snippet">#</a></h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Assume `data` is a time-series of experiences or observations</span>
</span></span><span style="display:flex;"><span>window_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">100</span>  <span style="color:#75715e"># Define the size of your window</span>
</span></span><span style="display:flex;"><span>current_window <span style="color:#f92672">=</span> data[<span style="color:#f92672">-</span>window_size:]  <span style="color:#75715e"># Keeps only the most recent data</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Train your model on `current_window`</span>
</span></span></code></pre></div><h3 id="3-online-learning-and-continuous-adaptation">3. Online Learning and Continuous Adaptation<a hidden class="anchor" aria-hidden="true" href="#3-online-learning-and-continuous-adaptation">#</a></h3>
<p>Online learning techniques, where the model is continuously updated with new data, can be particularly effective in non-stationary environments. This allows the model to adapt to changes on-the-fly.</p>
<h4 id="example-with-scikit-learn">Example with scikit-learn:<a hidden class="anchor" aria-hidden="true" href="#example-with-scikit-learn">#</a></h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.linear_model <span style="color:#f92672">import</span> SGDRegressor
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Create the model</span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> SGDRegressor()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Assume X_train, y_train are your features and labels respectively</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> x, y <span style="color:#f92672">in</span> zip(X_train, y_train):
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">.</span>partial_fit(x<span style="color:#f92672">.</span>reshape(<span style="color:#ae81ff">1</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>), [y])  <span style="color:#75715e"># Update the model incrementally</span>
</span></span></code></pre></div><h3 id="4-change-detection-mechanisms">4. Change Detection Mechanisms<a hidden class="anchor" aria-hidden="true" href="#4-change-detection-mechanisms">#</a></h3>
<p>Implementing a mechanism to detect changes in the environment can help trigger adaptation strategies. This can range from simple statistical tests to more complex machine learning models designed to identify shifts in data distribution.</p>
<h4 id="example-pseudocode">Example Pseudocode:<a hidden class="anchor" aria-hidden="true" href="#example-pseudocode">#</a></h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Assume `data_stream` is a generator yielding new observations</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> observation <span style="color:#f92672">in</span> data_stream:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> change_detected(observation):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Trigger model retraining or adaptation</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">pass</span>
</span></span></code></pre></div><h3 id="5-multi-model-approaches">5. Multi-Model Approaches<a hidden class="anchor" aria-hidden="true" href="#5-multi-model-approaches">#</a></h3>
<p>Using a portfolio of models, each trained on different segments of the data or under different assumptions, can provide a robust response to changing environments. This approach often leverages ensemble techniques.</p>
<h4 id="example-concept">Example Concept:<a hidden class="anchor" aria-hidden="true" href="#example-concept">#</a></h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Assume `models` is a list of different RL models</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># `data_segment` is the current batch of data to make predictions on</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>predictions <span style="color:#f92672">=</span> [model<span style="color:#f92672">.</span>predict(data_segment) <span style="color:#66d9ef">for</span> model <span style="color:#f92672">in</span> models]
</span></span><span style="display:flex;"><span>final_prediction <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>mean(predictions, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)  <span style="color:#75715e"># An example of ensemble prediction</span>
</span></span></code></pre></div><h2 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h2>
<p>Adapting reinforcement learning models to non-stationary environments is crucial for real-world applications. This article has introduced several strategies, from adaptive learning rates and sliding window techniques to online learning, change detection, and multi-model approaches. While there is no one-size-fits-all solution, combining these strategies can significantly increase the resilience and adaptability of your RL models. Experimentation and continuous monitoring are key to finding the optimal approach for your specific application, ensuring your models can withstand the test of time and change.</p>
<p>In the journey of mastering RL in non-stationary environments, remember that the goal is to build systems capable of learning and evolving as the world changes around them. The future of reinforcement learning is undoubtedly exciting, with endless possibilities for innovation and improvement. Embrace the challenge, and let your models learn not just to play the game but to play it well, no matter how often the rules may change.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://lustrous-paprenjak-b7c3d8.netlify.app/tags/Reinforcement-Learning/">Reinforcement Learning</a></li>
      <li><a href="https://lustrous-paprenjak-b7c3d8.netlify.app/tags/Machine-Learning/">Machine Learning</a></li>
      <li><a href="https://lustrous-paprenjak-b7c3d8.netlify.app/tags/Advanced-Topic/">Advanced Topic</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://lustrous-paprenjak-b7c3d8.netlify.app/">Data Driven Discovery - D3</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
